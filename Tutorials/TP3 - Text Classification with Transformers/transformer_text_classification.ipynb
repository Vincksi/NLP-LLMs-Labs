{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "184b0734",
   "metadata": {},
   "source": [
    "## Training Transformer Models for Text Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "384b9262",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"HF_HUB_DISABLE_SYMLINKS_WARNING\"] = \"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3969d300",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d2e08fe",
   "metadata": {},
   "source": [
    "### Hugging Face Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91b55c54",
   "metadata": {},
   "source": [
    "To load the Hugging Face \"emotion\" dataset efficiently, use the `load_dataset()` function from the `datasets` library, specifying the dataset by name. The most widely used version on the Hugging Face Hub is `\"dair-ai/emotion\"`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fbb8d08",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text', 'label'],\n",
       "        num_rows: 16000\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['text', 'label'],\n",
       "        num_rows: 2000\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['text', 'label'],\n",
       "        num_rows: 2000\n",
       "    })\n",
       "})"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# Load the dataset\n",
    "emotions_set = load_dataset(\"dair-ai/emotion\")\n",
    "display(emotions_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b17c05f4",
   "metadata": {},
   "source": [
    "The `emotions_set` object behaves like a Python dictionary, with each key representing a data split *(such as `'train'`, `'validation'`, or `'test'`)*, and you can access any split using standard dictionary indexing, for example `emotions_set['train']`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a303600",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['text', 'label'],\n",
       "    num_rows: 16000\n",
       "})"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_dataset = emotions_set[\"train\"]\n",
    "display(train_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11091133",
   "metadata": {},
   "source": [
    "The split we accessed is an instance of the `Dataset` class, one of the core data structures in the Datasets library. For now, you can think of a `Dataset` as behaving much like a standard Python list: for example, you can query its length with `len(dataset)` to see how many examples it contains."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d58d9d30",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16000"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(len(train_dataset))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d64bc82",
   "metadata": {},
   "source": [
    "A `Dataset` also supports integer indexing, so you can retrieve a single example by its position, for example `dataset[0]` to get the first record."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5611d474",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': 'i didnt feel humiliated', 'label': 0}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(train_dataset[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1074b013",
   "metadata": {},
   "source": [
    "A single row of a `Dataset` is returned as a Python dictionary, where each key is a column name and each value contains the corresponding data for that row."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c73c1f1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['text', 'label']"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(train_dataset.column_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99bbb8b7",
   "metadata": {},
   "source": [
    "In this dataset, the values are the tweet text and its associated emotion label. This layout reflects that the library is built on top of [Apache Arrow](https://arrow.apache.org/), which uses a typed, columnar memory format that is more efficient than native Python objects for large datasets. You can inspect the underlying column types by checking the `features` attribute of a `Dataset` object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afa6f06c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'text': Value('string'), 'label': ClassLabel(names=['sadness', 'joy', 'love', 'anger', 'fear', 'surprise'])}\n"
     ]
    }
   ],
   "source": [
    "print(train_dataset.features)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a75b726d",
   "metadata": {},
   "source": [
    "In this dataset, the `text` column uses the `string` type, while the `label` column is stored as a `ClassLabel` feature that keeps track of the class names and their integer IDs. You can also retrieve multiple rows at once by using standard Python slicing, for example `dataset[0:5]` to access several consecutive examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc0e7c3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'text': ['i didnt feel humiliated', 'i can go from feeling so hopeless to so damned hopeful just from being around someone who cares and is awake', 'im grabbing a minute to post i feel greedy wrong', 'i am ever feeling nostalgic about the fireplace i will know that it is still on the property', 'i am feeling grouchy'], 'label': [0, 0, 3, 2, 3]}\n"
     ]
    }
   ],
   "source": [
    "print(train_dataset[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d56dad5",
   "metadata": {},
   "source": [
    "When you slice a `Dataset`, the dictionary values become lists of elements rather than single items, since each key now holds a batch of rows. You can also retrieve an entire column at once by passing its name, for example `dataset[\"text\"]` to get all tweet texts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01fdc83b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i didnt feel humiliated', 'i can go from feeling so hopeless to so damned hopeful just from being around someone who cares and is awake', 'im grabbing a minute to post i feel greedy wrong', 'i am ever feeling nostalgic about the fireplace i will know that it is still on the property', 'i am feeling grouchy']\n"
     ]
    }
   ],
   "source": [
    "print(train_dataset[\"text\"][:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17ea02f7",
   "metadata": {},
   "source": [
    "#### Exercise 1: Dataset Exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb442adc",
   "metadata": {},
   "source": [
    "1. Browse the [Hugging Face Datasets Hub](https://huggingface.co/datasets) and find another text classification dataset (e.g., `imdb`, `ag_news`, or `yelp_review_full`). Load this dataset and explore its structure. How many classes does it have? How is it different from the emotion dataset?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88d2f866",
   "metadata": {},
   "source": [
    "A good example of another text classification dataset on the Hub is the IMDb movie reviews dataset. It contains 50,000 English reviews labeled for binary sentiment *(positive vs. negative)*, making it a standard benchmark for sentiment classification tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2204ee3a",
   "metadata": {},
   "source": [
    "Key differences from the emotion dataset\n",
    "- **Label space:** IMDb predicts coarse sentiment polarity *(positive vs. negative)*, while the emotion dataset predicts one of six finer-grained emotions.\n",
    "\n",
    "- **Domain:** IMDb consists of long-form movie reviews, whereas the emotion dataset contains short social media texts *(e.g., tweets)*.\n",
    "\n",
    "- **Task type:** Both are text classification tasks, but IMDb is standard sentiment analysis, and the emotion dataset targets emotion recognition, which is often considered more fine-grained and nuanced."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdce7018",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text', 'label'],\n",
       "        num_rows: 25000\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['text', 'label'],\n",
       "        num_rows: 25000\n",
       "    })\n",
       "    unsupervised: Dataset({\n",
       "        features: ['text', 'label'],\n",
       "        num_rows: 50000\n",
       "    })\n",
       "})"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load the IMDb dataset from the Hub\n",
    "imdb_set = load_dataset(\"imdb\")\n",
    "display(imdb_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6165ee6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'text': Value('string'), 'label': ClassLabel(names=['neg', 'pos'])}\n"
     ]
    }
   ],
   "source": [
    "# Inspect the features (column names and types) of the train split\n",
    "print(imdb_set[\"train\"].features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edc11a9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train size: 25000\n",
      "Test size: 25000\n",
      "Unsupervised size: 50000\n"
     ]
    }
   ],
   "source": [
    "# Check how many examples are in each split\n",
    "print(\"Train size:\", len(imdb_set[\"train\"]))\n",
    "print(\"Test size:\", len(imdb_set[\"test\"]))\n",
    "print(\"Unsupervised size:\", len(imdb_set[\"unsupervised\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7865eb3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'text': 'I rented I AM CURIOUS-YELLOW from my video store because of all the controversy that surrounded it when it was first released in 1967. I also heard that at first it was seized by U.S. customs if it ever tried to enter this country, therefore being a fan of films considered \"controversial\" I really had to see this for myself.<br /><br />The plot is centered around a young Swedish drama student named Lena who wants to learn everything she can about life. In particular she wants to focus her attentions to making some sort of documentary on what the average Swede thought about certain political issues such as the Vietnam War and race issues in the United States. In between asking politicians and ordinary denizens of Stockholm about their opinions on politics, she has sex with her drama teacher, classmates, and married men.<br /><br />What kills me about I AM CURIOUS-YELLOW is that 40 years ago, this was considered pornographic. Really, the sex and nudity scenes are few and far between, even then it\\'s not shot like some cheaply made porno. While my countrymen mind find it shocking, in reality sex and nudity are a major staple in Swedish cinema. Even Ingmar Bergman, arguably their answer to good old boy John Ford, had sex scenes in his films.<br /><br />I do commend the filmmakers for the fact that any sex shown in the film is shown for artistic purposes rather than just to shock people and make money to be shown in pornographic theaters in America. I AM CURIOUS-YELLOW is a good film for anyone wanting to study the meat and potatoes (no pun intended) of Swedish cinema. But really, this film doesn\\'t have much of a plot.', 'label': 0}\n"
     ]
    }
   ],
   "source": [
    "# Look at a single example\n",
    "print(imdb_set[\"train\"][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e91cc2cf",
   "metadata": {},
   "source": [
    "2. The current dataset is imbalanced. Using the Pandas documentation, research and implement at least one strategy to handle class imbalance *(e.g., using `resample()` or `sample()` with weights)*. What effect do you expect this to have on model performance?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f82f76c1",
   "metadata": {},
   "source": [
    "A straightforward way to address class imbalance in `dair-ai/emotion` is to randomly undersample the majority classes using `pandas.DataFrame.sample`, so that each label has a similar number of examples. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "781f2992",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label_name\n",
      "joy         5362\n",
      "sadness     4666\n",
      "anger       2159\n",
      "fear        1937\n",
      "love        1304\n",
      "surprise     572\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = train_dataset.to_pandas()  # columns: 'text', 'label' (int)\n",
    "\n",
    "# Map integer labels to human-readable names\n",
    "label_names = train_dataset.features[\"label\"].names\n",
    "df[\"label_name\"] = df[\"label\"].map(dict(enumerate(label_names)))\n",
    "\n",
    "# Check original class distribution\n",
    "print(df[\"label_name\"].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7b417b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label_name\n",
      "anger       572\n",
      "fear        572\n",
      "joy         572\n",
      "love        572\n",
      "sadness     572\n",
      "surprise    572\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Compute target size = size of smallest class\n",
    "min_count = df[\"label_name\"].value_counts().min()\n",
    "\n",
    "# Undersample each class equally using groupby.sample()\n",
    "df_undersampled = (\n",
    "    df.groupby(\"label_name\", group_keys=False)\n",
    "      .sample(n=min_count, random_state=42)\n",
    ")\n",
    "\n",
    "# Check new class distribution\n",
    "print(df_undersampled[\"label_name\"].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00fffb3d",
   "metadata": {},
   "source": [
    "This reduces the dominance of frequent emotions and encourages the model to pay more attention to rare classes, which typically improves recall and macro-averaged F1 for minority emotions such as *love* or *suprise*, because the decision boundary is no longer overwhelmed by majority samples.\n",
    "\n",
    "However, undersampling discards a substantial portion of the majority-class data, which can remove useful information, leading to weaker performance on those frequent emotions and sometimes a drop in overall accuracy or micro-F1, especially if the original dataset was not very large."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c273ddb",
   "metadata": {},
   "source": [
    "#### Offline or Local Datasets (Not on the Hugging Face Hub)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6115755",
   "metadata": {},
   "source": [
    "Most examples in this notebook use datasets downloaded directly from the Hugging Face Hub, but in practice you’ll often work with files stored locally (on your laptop) or on a private server.\n",
    "\n",
    "Datasets supports this via specialized loading scripts for common formats; we simply call `load_dataset()` with the appropriate builder (for example `\"csv\"`, `\"json\"`, or `\"parquet\"`) and pass a `data_files` argument pointing to one or more local paths or URLs. For instance, since the raw `dair-ai/emotion` data files live on Dropbox, we could bypass the Hub by downloading a split to disk and then loading it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f287c33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Download : train.txt\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "url = \"https://huggingface.co/datasets/transformersbook/emotion-train-split/raw/main/train.txt\"\n",
    "output_path = \"train.txt\"\n",
    "\n",
    "r = requests.get(url)\n",
    "r.raise_for_status()\n",
    "\n",
    "with open(output_path, \"wb\") as f:\n",
    "    f.write(r.content)\n",
    "\n",
    "print(\"Download :\", output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a06de4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i didnt feel humiliated;sadness\n",
      "\n"
     ]
    }
   ],
   "source": [
    "with open(\"train.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    first_line = f.readline()\n",
    "\n",
    "print(first_line)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5d6be7c",
   "metadata": {},
   "source": [
    "The file has no column headers, and each tweet–emotion pair is separated by a semicolon. However, its structure is close enough to a CSV that we can load it locally with the CSV loader, simply by passing the `csv` script and setting `data_files` to point to `train.txt`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e3c6e89",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3e4da2bac1a44a68a48f7fd10f39d111",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load the local dataset\n",
    "emotions_local = load_dataset(\"csv\", data_files=\"train.txt\", sep=\";\", \n",
    "                              names=[\"text\", \"label\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8373f39",
   "metadata": {},
   "source": [
    "Here, the delimiter and column names are explicitly specified. An even simpler option is to skip the local file altogether and pass the dataset URL directly to the `data_files` argument. This automatically downloads and caches the file for us."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0117aa11",
   "metadata": {},
   "source": [
    "#### From Datasets to DataFrames"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c80bb2e7",
   "metadata": {},
   "source": [
    "Datasets offers powerful tools for slicing and transforming data, but it is often more convenient to convert a `Dataset` to a Pandas `DataFrame` to take advantage of higher-level analysis and visualization APIs. The `set_format()` method lets you change the output format of a Dataset (for example, to `\"pandas\"`), without altering the underlying storage format, which remains an Apache Arrow table and can be switched to a different format again later if needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "054498fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>i didnt feel humiliated</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>i can go from feeling so hopeless to so damned...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>im grabbing a minute to post i feel greedy wrong</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>i am ever feeling nostalgic about the fireplac...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>i am feeling grouchy</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  label\n",
       "0                            i didnt feel humiliated      0\n",
       "1  i can go from feeling so hopeless to so damned...      0\n",
       "2   im grabbing a minute to post i feel greedy wrong      3\n",
       "3  i am ever feeling nostalgic about the fireplac...      2\n",
       "4                               i am feeling grouchy      3"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "emotions_set.set_format(type=\"pandas\")\n",
    "df = emotions_set[\"train\"][:]\n",
    "\n",
    "display(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38af5d62",
   "metadata": {},
   "source": [
    "The column names are preserved, and the first rows line up with what we saw before. Since the labels are still stored as integers, we can call the `int2str()` method of the label feature to add a new column to the `DataFrame` containing the corresponding label names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b916af33",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "      <th>label_name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>i didnt feel humiliated</td>\n",
       "      <td>0</td>\n",
       "      <td>sadness</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>i can go from feeling so hopeless to so damned...</td>\n",
       "      <td>0</td>\n",
       "      <td>sadness</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>im grabbing a minute to post i feel greedy wrong</td>\n",
       "      <td>3</td>\n",
       "      <td>anger</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>i am ever feeling nostalgic about the fireplac...</td>\n",
       "      <td>2</td>\n",
       "      <td>love</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>i am feeling grouchy</td>\n",
       "      <td>3</td>\n",
       "      <td>anger</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  label label_name\n",
       "0                            i didnt feel humiliated      0    sadness\n",
       "1  i can go from feeling so hopeless to so damned...      0    sadness\n",
       "2   im grabbing a minute to post i feel greedy wrong      3      anger\n",
       "3  i am ever feeling nostalgic about the fireplac...      2       love\n",
       "4                               i am feeling grouchy      3      anger"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def label_int2str(row):\n",
    "    return emotions_set[\"train\"].features[\"label\"].int2str(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbd52076",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Map integer labels to human-readable names\n",
    "df[\"label_name\"] = df[\"label\"].apply(label_int2str)\n",
    "display(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5a78483",
   "metadata": {},
   "source": [
    "Before jumping into model building, it’s worth spending some time exploring the dataset in detail. As Andrej Karpathy emphasizes in his blog post [A Recipe for Training Neural Networks](https://karpathy.github.io/2019/04/25/recipe/), truly understanding your data (“becoming one with the data”) is a crucial step toward training strong models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "766fcae6",
   "metadata": {},
   "source": [
    "#### Class Distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "333cd33e",
   "metadata": {},
   "source": [
    "When working on text classification tasks, it is important to inspect how examples are distributed across classes. If the class distribution is highly skewed, you may need to adjust your choice of loss function and evaluation metrics compared to a balanced dataset. Using Pandas together with Matplotlib, you can quickly plot this class distribution to get an immediate visual impression of any imbalance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3295468",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Plot class distribution\n",
    "class_counts = df[\"label_name\"].value_counts(ascending=True)\n",
    "\n",
    "plt.figure(figsize=(8, 5))\n",
    "class_counts.plot(kind='barh', color='skyblue', edgecolor='black')\n",
    "plt.title(\"Frequency of Classes\", fontsize=14)\n",
    "plt.xlabel(\"Count\", fontsize=12)\n",
    "plt.ylabel(\"Class\", fontsize=12)\n",
    "plt.grid(axis='x', linestyle='--', alpha=0.7)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2830adf",
   "metadata": {},
   "source": [
    "In this dataset, the class distribution is strongly imbalanced: `joy` and `sadness` are very frequent, while `love` and `surprise` appear roughly 5–10 times less often. This kind of skew can be handled in several ways, but in this chapter we will keep the original, unbalanced class frequencies and train on them directly *(see Exercice 1)*.\n",
    "\n",
    "Common strategies for imbalanced data include:\n",
    "- Randomly oversampling minority classes.\n",
    "- Randomly undersampling majority classes.\n",
    "- Collecting more labeled examples for underrepresented classes.\n",
    "\n",
    "To explore these techniques in more depth, you can look at the Imbalanced-learn library, which provides many resampling methods and utilities for imbalanced datasets.\n",
    "\n",
    "When applying any sampling strategy, it is crucial to do it after creating your train/test (and validation) splits; otherwise, resampled examples can leak information across splits and lead to overly optimistic evaluation results."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2c933b3",
   "metadata": {},
   "source": [
    "#### Tweet Length Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9af12195",
   "metadata": {},
   "source": [
    "Transformer models have a maximum input sequence length, often called the **maximum context size**. For DistilBERT, this limit is 512 tokens, which corresponds to roughly a few paragraphs of text. In the next section, a **token** will be defined more precisely, but for now you can think of it roughly as a word. To approximate how long tweets are for each emotion, we can examine the distribution of word counts per tweet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "889ace36",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot boxplot of words per tweet by class\n",
    "df[\"Words Per Tweet\"] = df[\"text\"].str.split().apply(len)\n",
    "\n",
    "plt.figure(figsize=(8, 5))\n",
    "df.boxplot(\n",
    "    column=\"Words Per Tweet\",\n",
    "    by=\"label_name\",\n",
    "    grid=False,\n",
    "    showfliers=False,\n",
    "    patch_artist=True,\n",
    "    boxprops=dict(facecolor=\"lightblue\", color=\"black\"),\n",
    "    medianprops=dict(color=\"red\")\n",
    ")\n",
    "\n",
    "plt.title(\"Words per Tweet by Class\", fontsize=14)\n",
    "plt.suptitle(\"\")  # Supprime le titre par défaut de pandas\n",
    "plt.xlabel(\"Class\", fontsize=12)\n",
    "plt.ylabel(\"Number of Words\", fontsize=12)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b86af816",
   "metadata": {},
   "source": [
    "The plot shows that, across all emotions, most tweets are roughly 15 words long, and even the longest tweets remain comfortably below DistilBERT’s maximum context window. When inputs exceed a model’s context size they must be truncated, which can hurt performance if important information is cut off. Here, the tweet lengths are short enough that truncation should not be a concern."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14a6b244",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reset dataset format to default\n",
    "emotions_set.reset_format()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0e22dd3",
   "metadata": {},
   "source": [
    "### From Text to Tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bea36328",
   "metadata": {},
   "source": [
    "Transformer models such as DistilBERT do not process raw text directly, instead they require input that has been **tokenized** and **encoded** into numerical vectors. **Tokenization** refers to the process of dividing text into the fundamental units that the model can understand. Various tokenization strategies exist, and the optimal way of splitting words into smaller subunits is typically learned from the training corpus."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7edfeb22",
   "metadata": {},
   "source": [
    "#### Character Tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3e1dcbe",
   "metadata": {},
   "source": [
    "The most basic tokenization strategy is **character-level tokenization**, where each individual character is treated as a separate token. In Python, strings behave like sequences of characters, so a character-level tokenizer can be implemented very concisely by iterating directly over the string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ac2c35e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['T', 'o', 'k', 'e', 'n', 'i', 'z', 'i', 'n', 'g', ' ', 't', 'e', 'x', 't', ' ', 'i', 's', ' ', 'a', ' ', 'c', 'o', 'r', 'e', ' ', 't', 'a', 's', 'k', ' ', 'o', 'f', ' ', 'N', 'L', 'P', '.']\n"
     ]
    }
   ],
   "source": [
    "text = \"Tokenizing text is a core task of NLP.\"\n",
    "tokenized_text = list(text)\n",
    "\n",
    "print(tokenized_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04734595",
   "metadata": {},
   "source": [
    "This is a reasonable first step, but it is not yet in the form the model requires. The model expects each character token to be mapped to an integer ID, a process often referred to as **numericalization**. A simple way to achieve this is to assign a unique integer index to every distinct token (here, each character) in the vocabulary and then replace each character in the text with its corresponding index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ecaa243",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{' ': 0, '.': 1, 'L': 2, 'N': 3, 'P': 4, 'T': 5, 'a': 6, 'c': 7, 'e': 8, 'f': 9, 'g': 10, 'i': 11, 'k': 12, 'n': 13, 'o': 14, 'r': 15, 's': 16, 't': 17, 'x': 18, 'z': 19}\n"
     ]
    }
   ],
   "source": [
    "token2idx = {ch: idx for idx, ch in enumerate(sorted(set(tokenized_text)))}\n",
    "print(token2idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f3fb285",
   "metadata": {},
   "source": [
    "This defines a unique integer ID for every character in the vocabulary, and we can now use `token2idx` to convert the tokenized text into a sequence of integer IDs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48c82e07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5, 14, 12, 8, 13, 11, 19, 11, 13, 10, 0, 17, 8, 18, 17, 0, 11, 16, 0, 6, 0, 7, 14, 15, 8, 0, 17, 6, 16, 12, 0, 14, 9, 0, 3, 2, 4, 1]\n"
     ]
    }
   ],
   "source": [
    "input_ids = [token2idx[token] for token in tokenized_text]\n",
    "print(input_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5956e3b2",
   "metadata": {},
   "source": [
    "Each token is now represented by a unique integer ID (hence the name `input_ids`). The next step is to convert these `input_ids` into a 2D tensor of one-hot vectors. One-hot encodings are a common way to represent categorical variables (whether ordinal or nominal) by assigning each category a distinct index and then representing it as a vector that is zero everywhere except at that index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69cdd1a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Name</th>\n",
       "      <th>Label ID</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Bumblebee</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Optimus Prime</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Megatron</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            Name  Label ID\n",
       "0      Bumblebee         0\n",
       "1  Optimus Prime         1\n",
       "2       Megatron         2"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "categorical_df = pd.DataFrame(\n",
    "    {\"Name\": [\"Bumblebee\", \"Optimus Prime\", \"Megatron\"], \"Label ID\": [0,1,2]})\n",
    "\n",
    "display(categorical_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d88250a8",
   "metadata": {},
   "source": [
    "This encoding scheme has a drawback: by assigning consecutive integers to categories, it implicitly introduces an artificial ordering among them, which neural networks are very good at exploiting even though it is meaningless. Instead, we can represent each category with its own binary feature, setting the value to 1 when the category is present and 0 otherwise. In Pandas, this one-hot representation can be created conveniently with the `get_dummies()` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c25e2f73",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Bumblebee</th>\n",
       "      <th>Megatron</th>\n",
       "      <th>Optimus Prime</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Bumblebee  Megatron  Optimus Prime\n",
       "0          1         0              0\n",
       "1          0         0              1\n",
       "2          0         1              0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dummies = pd.get_dummies(categorical_df[\"Name\"]).astype(int)\n",
    "display(dummies)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "099ec97e",
   "metadata": {},
   "source": [
    "Each row of this `DataFrame` is a one-hot vector, with exactly one position set to 1 (the “hot” entry) and all others set to 0. In contrast, the `input_ids` form an artificial ordinal scale: treating these IDs arithmetically (for example, by adding or subtracting them) is meaningless, because the resulting number is just another token ID with no numeric relationship to the originals."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b45ae5e",
   "metadata": {},
   "source": [
    "Adding two one-hot vectors, by contrast, has an intuitive meaning: the positions with a value of 1 indicate which tokens are present, so their sum directly reflects co-occurrence. To build such one-hot encodings in PyTorch, you first convert `input_ids` to a tensor and then apply `torch.nn.functional.one_hot()` to obtain a 2D tensor of one-hot vectors for the sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "719c7804",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([38, 20])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "input_ids = torch.tensor(input_ids)\n",
    "one_hot_encodings = F.one_hot(input_ids, num_classes=len(token2idx))\n",
    "\n",
    "display(one_hot_encodings.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03850556",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token: T\n",
      "Tensor index: 5\n",
      "One-hot: tensor([0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n"
     ]
    }
   ],
   "source": [
    "print(f\"Token: {tokenized_text[0]}\")\n",
    "print(f\"Tensor index: {input_ids[0]}\")\n",
    "print(f\"One-hot: {one_hot_encodings[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0d22dac",
   "metadata": {},
   "source": [
    "Our simple example illustrates that **character-level tokenization** disregards the inherent structure of text, interpreting it purely as a sequence of characters. While this approach can effectively handle misspellings and rare words, it forces the model to **learn higher-level linguistic structures**, such as words, directly from the data. This learning process demands substantial computational resources, memory, and data, which is why character-level tokenization is seldom used in practice."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a0dea5c",
   "metadata": {},
   "source": [
    "#### Word Tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bf6d228",
   "metadata": {},
   "source": [
    "Rather than splitting the text into individual characters, we can divide it into words and assign each word a unique integer. Starting with words allows the model to bypass the process of deriving words from characters, thereby simplifying and accelerating training."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "900b9f0a",
   "metadata": {},
   "source": [
    "A straightforward type of **word tokenizer** relies on whitespace to separate tokens. In practice, this can be done by applying Python’s `split()` function directly to the raw text, just as we did when measuring tweet lengths.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b38db127",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Tokenizing', 'text', 'is', 'a', 'core', 'task', 'of', 'NLP.']\n"
     ]
    }
   ],
   "source": [
    "tokenized_text = text.split()\n",
    "print(tokenized_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0fef3f9",
   "metadata": {},
   "source": [
    "From this point, we can follow the same procedure used for the character tokenizer to assign an ID to each word. However, this tokenization approach introduces a potential issue: it does not handle punctuation, meaning that a term like `NLP.` is treated as a single token. Moreover, since words may appear in various forms due to declensions, conjugations, or misspellings, the vocabulary size can quickly expand to millions of entries.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd9804c1",
   "metadata": {},
   "source": [
    "A large vocabulary poses a significant challenge because it forces neural networks to maintain an enormous number of parameters. For example, imagine a model with one million unique words, where the first network layer reduces one-million-dimensional input vectors to one-thousand-dimensional representations. This common step in many NLP architectures would require a weight matrix containing 1 million × 1 thousand = 1 billion parameters, already comparable to the largest GPT-2 model which has approximately 1.5 billion parameters in total.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33664fce",
   "metadata": {},
   "source": [
    "Naturally, we want to avoid wasting model parameters, as training large models is both computationally expensive and difficult to maintain. A common solution is to restrict the vocabulary to a fixed size (for example, the 100,000 most frequent words in the corpus) and discard rarer ones. Words outside this limited vocabulary are replaced with a shared `UNK` token, representing “unknown.” However, this approach inevitably leads to some information loss, since the model cannot learn meaningful representations for the words mapped to `UNK`.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "179591ca",
   "metadata": {},
   "source": [
    "#### Subword Tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aee5e708",
   "metadata": {},
   "source": [
    "**Subword tokenization** aims to capture the advantages of both character- and word-level tokenization. Rare or complex words are split into smaller pieces so the model can still represent misspellings and infrequent forms, while common words are kept as single tokens to keep sequences short and efficient. Unlike simple rule-based splitting, these subword vocabularies are learned from a large pretraining corpus using statistical algorithms."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fdc8666",
   "metadata": {},
   "source": [
    "A widely used subword algorithm is WordPiece, employed by the BERT and DistilBERT tokenizers. In practice, you typically load such a tokenizer via a high-level `AutoTokenizer` interface by calling its `from_pretrained()` method with the identifier of a pretrained model (for example, the DistilBERT model you want to use), which automatically gives you the corresponding WordPiece tokenizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "764917b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# hide_output\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "model_ckpt = \"distilbert-base-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_ckpt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b101b354",
   "metadata": {},
   "source": [
    "The `AutoTokenizer` class is part of a broader family of “auto” utilities that can automatically load the right configuration, weights, and vocabulary given just a checkpoint name, making it easy to swap models without changing much code. If you prefer, you can also instantiate the concrete tokenizer class directly instead of going through the auto interface. For DistilBERT, we could use `DistilBertTokenizerFast.from_pretrained(...)` to load the same tokenizer explicitly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "218b2b04",
   "metadata": {},
   "outputs": [],
   "source": [
    "#from transformers import DistilBertTokenizer\n",
    "#distilbert_tokenizer = DistilBertTokenizer.from_pretrained(model_ckpt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b978ed58",
   "metadata": {},
   "source": [
    "To see how this tokenizer behaves in practice, let’s apply it to our example sentence: *Tokenizing text is a core task of NLP.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07288102",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': [101, 19204, 6026, 3793, 2003, 1037, 4563, 4708, 1997, 17953, 2361, 1012, 102], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n"
     ]
    }
   ],
   "source": [
    "encoded_text = tokenizer(text)\n",
    "print(encoded_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "474d4276",
   "metadata": {},
   "source": [
    "As with character-level tokenization, the tokenizer has mapped each token to a unique integer ID in the `input_ids` field. The purpose of the `attention_mask` field will be explained in the next section. Given these `input_ids`, we can recover the corresponding tokens by calling the tokenizer’s `convert_ids_to_tokens()` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01369f79",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = tokenizer.convert_ids_to_tokens(encoded_text.input_ids)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d1c11de",
   "metadata": {},
   "source": [
    "We can notice three key details here. First, special tokens such as `[CLS]` and `[SEP]` have been added at the beginning and end of the sequence. Although the exact tokens vary across models, their primary purpose is to mark the sequence boundaries. Second, all tokens have been converted to lowercase, which is specific to this particular model checkpoint. Finally, words like “tokenizing” and “NLP” have been split into multiple tokens—an expected outcome since they are relatively uncommon. The `##` prefix in `##izing` and `##p` indicates that the token follows another without an intervening space, meaning it should be merged with the preceding token when reconstructing text. The `AutoTokenizer` class provides a `convert_tokens_to_string()` method to perform this reassembly automatically. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d580f41c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tokenizer.convert_tokens_to_string(tokens))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2349eddd",
   "metadata": {},
   "source": [
    "The `AutoTokenizer` class also has several attributes that provide information about the tokenizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f992e52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the vocabulary size\n",
    "display(tokenizer.vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccf273e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the maximum model input length\n",
    "display(tokenizer.model_max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21860b80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the model input names\n",
    "display(tokenizer.model_input_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c12a5859",
   "metadata": {},
   "source": [
    "#### Tokenizing the Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03844837",
   "metadata": {},
   "source": [
    "To tokenize the entire corpus, we will use the `map()` method of the `DatasetDict` object. This method will appear frequently throughout the notebook, as it offers a convenient way to apply a processing function to every element in a dataset. As we will see shortly, `map()` can also be used to generate new rows and columns within the dataset.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c342bab5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(batch):\n",
    "    return tokenizer(batch[\"text\"], padding=True, truncation=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7294ff51",
   "metadata": {},
   "source": [
    "This function applies the tokenizer to a batch of examples. Setting `padding=True` pads each example with zeros to match the length of the longest sequence in the batch, while `truncation=True` ensures that sequences exceeding the model’s maximum context length are truncated accordingly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63f04aa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tokenize(emotions_set[\"train\"][:2]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8ac0d5a",
   "metadata": {},
   "source": [
    "Here, we can observe the effect of padding: since the first element of input_ids is shorter than the second, zeros have been added to match their lengths. These zeros correspond to the `[PAD]` token in the vocabulary, while the set of special tokens also includes the `[CLS]` and `[SEP]` tokens introduced earlier.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2e8f186",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pair each special token with its corresponding ID\n",
    "tokens_to_ids = list(zip(tokenizer.all_special_tokens, tokenizer.all_special_ids))\n",
    "\n",
    "# Sort by token ID\n",
    "data = sorted(tokens_to_ids, key=lambda x: x[-1])\n",
    "\n",
    "# Create and display a DataFrame\n",
    "df = pd.DataFrame(data, columns=[\"Special Token\", \"Special Token ID\"])\n",
    "display(df.T)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a299e7b0",
   "metadata": {},
   "source": [
    "Note that, in addition to providing the encoded tweets as `input_ids`, the tokenizer also returns a list of `attention_mask` arrays. These masks prevent the model from being influenced by the padding tokens, ensuring that only the meaningful parts of each sequence are attended to. Within each batch, input sequences are padded to the length of the longest sequence, and the attention mask guides the model to disregard the padded regions during processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c52c8847",
   "metadata": {},
   "outputs": [],
   "source": [
    "emotions_encoded = emotions_set.map(tokenize, batched=True, batch_size=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef228ef0",
   "metadata": {},
   "source": [
    "By default, the `map()` method processes each example in the dataset individually. Setting batched=True enables batch processing, allowing the tweets to be encoded in groups. Since we have specified `batch_size=None`, the `tokenize()` function is applied to the entire dataset as a single batch. This approach ensures that the resulting input tensors and attention masks have consistent shapes across the dataset, and it also adds new `input_ids` and `attention_mask` columns to the dataset.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23722835",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(emotions_encoded[\"train\"].column_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c6f9dd0",
   "metadata": {},
   "source": [
    "### Training a Text Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a3e7902",
   "metadata": {},
   "source": [
    "The process begins by tokenizing the text and representing it as one-hot vectors, known as **token encodings**. The dimensionality of these encodings is determined by the size of the tokenizer’s vocabulary, which typically ranges from 20,000 to 200,000 unique tokens. These token encodings are then transformed into token embeddings—dense vectors situated in a lower-dimensional space. The **embeddings** are passed through the encoder block layers, producing a hidden state for each input token. During pretraining for language modeling, each hidden state is used to predict the masked tokens. For **classification tasks**, however, the language modeling head is replaced with a classification layer.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "094d0b2f",
   "metadata": {},
   "source": [
    "We have two main strategies for training this type of model on our Twitter dataset:\n",
    "\n",
    "- **Feature extraction:** Use the hidden states as fixed feature representations and train a separate classifier on top of them, leaving the pretrained model unchanged.  \n",
    "- **Fine-tuning:** Train the entire model end-to-end, allowing the parameters of the pretrained model to be updated during training.  \n",
    "\n",
    "In the following sections, we’ll explore both approaches using DistilBERT and discuss the trade-offs between them.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88f547fb",
   "metadata": {},
   "source": [
    "#### Transformers as Feature Extractors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63a767e4",
   "metadata": {},
   "source": [
    "Using a transformer as a **feature extractor** is straightforward: we freeze the model’s body during training and use its **hidden states** as input features for a classifier. This approach allows for training smaller or shallower models efficiently. The classifier can be a simple neural layer or even a non-gradient-based method, such as a random forest. Moreover, this method is particularly practical when GPUs are unavailable, since the hidden states only need to be computed once."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aab6b62c",
   "metadata": {},
   "source": [
    "We will make use of another convenient auto class from the Transformers library, called `AutoModel`. Like `AutoTokenizer`, it provides a `from_pretrained()` method that loads the weights of a pretrained model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "918f4169",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModel\n",
    "\n",
    "model_ckpt = \"distilbert-base-uncased\"\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = AutoModel.from_pretrained(model_ckpt).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b40bbd3a",
   "metadata": {},
   "source": [
    "#### Exercise 2: Tokenization Experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2316cdab",
   "metadata": {},
   "source": [
    "1. Load a different pre-trained model tokenizer (`bert-base-cased` or `roberta-base`) and compare its tokenization output with DistilBERT.  Use the [Hugging Face Models Hub](https://huggingface.co/models) to find available models. What differences do you notice? How does cased vs uncased tokenization affect the output?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54578111",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the tokenizers\n",
    "distilbert_tok = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "roberta_tok    = AutoTokenizer.from_pretrained(\"roberta-base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84026e1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_text = \"Tokenizing NLP with DistilBERT and RoBERTa is Fun!\"\n",
    "\n",
    "print(\"DistilBERT:\", distilbert_tok.tokenize(sample_text))\n",
    "print(\"RoBERTa:  \", roberta_tok.tokenize(sample_text))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d3c8800",
   "metadata": {},
   "source": [
    "For this sentence, DistilBERT and RoBERTa differ in how they split words, how they mark subwords, and how they handle whitespace and casing.\n",
    "\n",
    "- DistilBERT uses **WordPiece** with `##` to mark subword continuations, producing tokens such as `['token', '##izing']`, `['nl', '##p']`, and `['di', '##sti', '##lbert']`.\n",
    "\n",
    "- RoBERTa uses **byte‑level BPE** and a special `Ġ` prefix to mark tokens that start with a space, yielding tokens like `['Token', 'izing']` , `['ĠN', 'LP']`, `['ĠDist', 'il', 'BER', 'T']`, and `['ĠRo', 'BER', 'Ta']`.\n",
    "\n",
    "DistilBERT’s output is more “word‑like” with clear base‑plus‑suffix structure, whereas RoBERTa’s output reflects finer‑grained byte‑level splits and explicit whitespace markers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aef38c50",
   "metadata": {},
   "source": [
    "Cased vs uncased tokenization also has a clear effect on the outputs.\n",
    "\n",
    "- A **cased tokenizer** like RoBERTa preserves capitalization, so `Token` vs `token` or `Fun` vs `fun` become different tokens, allowing the model to leverage case as a signal (e.g. for acronyms, proper nouns, or emphasis).\n",
    "\n",
    "- An **uncased tokenizer** like DistilBERT maps different casing variants to the same subword pieces, which reduces vocabulary sparsity and is robust to noisy casing, but discards any information that is encoded purely by capitalization."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40137a86",
   "metadata": {},
   "source": [
    "2.  Experiment with the tokenizer's `padding` and `truncation` parameters. What happens if you set `max_length=10` with `truncation=True`? Read the [tokenizer documentation](https://huggingface.co/docs/transformers/main_classes/tokenizer) to understand different padding strategies (`max_length`, `longest` or `do_not_pad`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c554443",
   "metadata": {},
   "outputs": [],
   "source": [
    "# \n",
    "enc = tokenizer(\n",
    "    sample_text,\n",
    "    max_length=10,\n",
    "    truncation=True,\n",
    "    padding=\"max_length\",\n",
    ")\n",
    "\n",
    "print(tokenizer.convert_ids_to_tokens(enc.input_ids))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21ec1408",
   "metadata": {},
   "source": [
    "When `max_length=10` and `truncation=True` are set, any encoded sequence longer than 10 tokens is cut so that its final length is exactly 10 tokens, including any special tokens the tokenizer adds. Shorter sequences can then be padded depending on the chosen padding strategy.\n",
    "\n",
    "- Any sequence whose encoded length exceeds 10 tokens is truncated down to 10 tokens.\n",
    "\n",
    "- With `padding=\"max_length\"`, all shorter sequences are padded with the pad token up to length 10, so every `input_ids` vector has length 10."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ecf70a2",
   "metadata": {},
   "source": [
    "**Padding strategies**\n",
    "\n",
    "- `padding=\"max_length\"`\n",
    "\n",
    "    - Always pads each sequence to `max_length` (or to the model’s built‑in maximum if `max_length` is not provided).\n",
    "\n",
    "    - With `max_length=10`, all sequences end up with length 10, regardless of how long they were originally.\n",
    "\n",
    "- `padding=\"longest\"` or `padding=True`\n",
    "\n",
    "    - In each batch, pads sequences only up to the length of the longest encoded example in that batch.\n",
    "\n",
    "    - If `max_length=10` and `truncation=True` are also set, the longest sequence in the batch is at most 10 tokens, and shorter ones are padded up to that length (≤ 10).\n",
    "\n",
    "- `padding=\"do_not_pad\"` or `padding=False`\n",
    "\n",
    "    - No padding is added; each sequence keeps its own length after truncation.\n",
    "\n",
    "    - With `max_length=10` and `truncation=True`, all sequences have length at most 10, but lengths can differ between sequences, so they cannot be stacked into a single rectangular batch tensor without further processing."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fac0fca",
   "metadata": {},
   "source": [
    "#### Interoperability between Frameworks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7e9b208",
   "metadata": {},
   "source": [
    "Transformers is primarily demonstrated with PyTorch in this notebook, but it also integrates seamlessly with TensorFlow and JAX. In practice, only a few lines of code need to be modified to load and use a pretrained model in your preferred deep learning framework."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf4a0588",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TFAutoModel\n",
    "\n",
    "model_ckpt = \"bert-base-uncased\"\n",
    "tf_model = TFAutoModel.from_pretrained(model_ckpt, from_pt=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ee69b0a",
   "metadata": {},
   "source": [
    "This interoperability becomes particularly handy when a model is released in only one framework but you want to use it in another, such as with the **XLM-RoBERTa** model. In such cases, you can pass `from_pt=True` to `TFAutoModel.from_pretrained()`, and the library will automatically download the PyTorch checkpoint and convert the weights to TensorFlow for you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3ab1bce",
   "metadata": {},
   "outputs": [],
   "source": [
    "#tf_xlmr = TFAutoModel.from_pretrained(\"xlm-roberta-base\", from_pt=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36aac967",
   "metadata": {},
   "source": [
    "Switching between frameworks in Transformers is straightforward. In most cases, you can obtain the TensorFlow 2.0 equivalents simply by adding a `TF` prefix to the PyTorch class names. Likewise, when a configuration or method expects the string `\"pt\"` (for PyTorch), you can usually replace it with `\"tf\"` to select the TensorFlow variant instead."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a404f2f2",
   "metadata": {},
   "source": [
    "#### Extracting the Hidden States"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8202872",
   "metadata": {},
   "source": [
    "To begin, retrieve the **last hidden states** for a single input string. First encode the string and convert the resulting tokens into PyTorch tensors by passing `return_tensors=\"pt\"` to the tokenizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a49d0aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"this is a test\"\n",
    "\n",
    "inputs = tokenizer(text, return_tensors=\"pt\")\n",
    "print(f\"Input tensor shape: {inputs['input_ids'].size()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d97f2f7a",
   "metadata": {},
   "source": [
    "The encoded input tensor has shape `[batch_size, n_tokens]`, as expected. Once the encodings are in tensor form, the final step is to move them onto the same device as the model and then feed them to the model as inputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fe03af0",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = {k:v.to(device) for k,v in inputs.items()}\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)\n",
    "    \n",
    "print(outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3414397c",
   "metadata": {},
   "source": [
    "The code runs inside a `torch.no_grad()` context to disable gradient computation, which is ideal for inference because it lowers the memory usage. The model’s output can include multiple elements (such as hidden states, losses, or attention weights) packaged in an object similar to a Python `namedtuple`. In this case, the output is a `BaseModelOutput` instance, so its fields can be accessed by name: for the current configuration it exposes only a single field, the last hidden state, whose tensor shape we can now inspect."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81fc050e",
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs.last_hidden_state.size()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6276010",
   "metadata": {},
   "source": [
    "The hidden state tensor has shape `[batch_size, n_tokens, hidden_dim]`, so in this example there is a 768‑dimensional vector for each of the 6 input tokens. For classification tasks, it is common to use only the hidden state corresponding to the `[CLS]` token as the sequence representation; because `[CLS]` is placed at the beginning of the sequence, it can be extracted by taking the first token along the sequence dimension of `outputs.last_hidden_state`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4445a521",
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs.last_hidden_state[:,0].size()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49109456",
   "metadata": {},
   "source": [
    "Now that the procedure for obtaining the last hidden state for a single string is clear, the next step is to apply it to the entire dataset by creating a new `hidden_state` column that stores these vectors. As with tokenization, this can be done efficiently by using the `map()` method of `DatasetDict` to compute and attach all hidden states in a single pass."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53a968a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_hidden_states(batch):\n",
    "    # Place model inputs on the CPU\n",
    "    inputs = {k:v.to(device) for k,v in batch.items() \n",
    "              if k in tokenizer.model_input_names}\n",
    "    # Extract last hidden states\n",
    "    with torch.no_grad():\n",
    "        last_hidden_state = model(**inputs).last_hidden_state\n",
    "    # Return vector for [CLS] token\n",
    "    return {\"hidden_state\": last_hidden_state[:,0].cpu().numpy()}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34c018ef",
   "metadata": {},
   "source": [
    "The only change compared to the previous logic is the final step, where the last hidden state is moved back to the CPU and converted to a NumPy array. When using batched inputs, the `map()` method expects the processing function to return standard Python or NumPy objects."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58402bf5",
   "metadata": {},
   "source": [
    "The model expects tensor inputs, so the next step is to cast the `input_ids` and `attention_mask` columns to the `\"torch\"` format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0b19917",
   "metadata": {},
   "outputs": [],
   "source": [
    "emotions_encoded.set_format(\"torch\", \n",
    "                            columns=[\"input_ids\", \"attention_mask\", \"label\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "867868f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "emotions_hidden = emotions_encoded.map(extract_hidden_states, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "937f091c",
   "metadata": {},
   "outputs": [],
   "source": [
    "emotions_hidden[\"train\"].column_names"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9e6fc23",
   "metadata": {},
   "source": [
    "#### Creating a Feature Matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db7c0d00",
   "metadata": {},
   "source": [
    "The preprocessed dataset now contains everything required to train a classifier. The hidden states will serve as input features, and the labels will act as targets. From this, it is straightforward to construct feature and label arrays in the standard Scikit-Learn format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49e66fce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Convert hidden states and labels to NumPy arrays\n",
    "X_train = np.asarray(emotions_hidden[\"train\"][\"hidden_state\"])\n",
    "X_valid = np.asarray(emotions_hidden[\"validation\"][\"hidden_state\"])\n",
    "\n",
    "y_train = np.asarray(emotions_hidden[\"train\"][\"label\"])\n",
    "y_valid = np.asarray(emotions_hidden[\"validation\"][\"label\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "245f981e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display shapes\n",
    "print(\"X_train shape:\", X_train.shape)\n",
    "print(\"X_valid shape:\", X_valid.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "938f6369",
   "metadata": {},
   "source": [
    "Before training a model on these hidden states, it is good practice to run a **sanity check** to verify that they actually capture the emotion structure we care about. This will be done by visualizing the **feature space**, which provides a quick way to assess how well the emotions are separated."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a61fcc72",
   "metadata": {},
   "source": [
    "#### Visualizing the Training Set"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21b5223a",
   "metadata": {},
   "source": [
    "Visualizing 768-dimensional hidden states directly is impractical, so the vectors will be projected into 2D using the **UMAP** algorithm. Before applying UMAP, the features will be rescaled to the [0,1] range with a `MinMaxScaler`, since UMAP typically performs better on normalized inputs, and then the `umap-learn` implementation will be used to compute the 2D embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "707f12cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# Scale features to the [0, 1] range\n",
    "scaler = MinMaxScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28aa3c8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from umap import UMAP\n",
    "\n",
    "# Fit UMAP for 2D projection\n",
    "umap_model = UMAP(\n",
    "    n_components=2,\n",
    "    metric=\"cosine\",\n",
    "    random_state=42\n",
    ")\n",
    "X_train_embedded = umap_model.fit_transform(X_train_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24696897",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build a DataFrame with embeddings + labels\n",
    "df_emb = pd.DataFrame(\n",
    "    X_train_embedded,\n",
    "    columns=[\"X\", \"Y\"]\n",
    ")\n",
    "df_emb[\"label\"] = y_train\n",
    "\n",
    "display(df_emb.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2db541d2",
   "metadata": {},
   "source": [
    "The transformed array contains the same number of training examples, but each sample is now represented by just 2 features instead of the original 768."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e334096c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare subplot grid\n",
    "fig, axes = plt.subplots(\n",
    "    nrows=2, ncols=3,\n",
    "    figsize=(7, 5),\n",
    "    constrained_layout=True\n",
    ")\n",
    "axes = axes.flatten()\n",
    "\n",
    "cmaps = [\"Greys\", \"Blues\", \"Oranges\", \"Reds\", \"Purples\", \"Greens\"]\n",
    "labels = emotions_set[\"train\"].features[\"label\"].names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25663dc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot hexbin for each emotion category\n",
    "for idx, (label_name, cmap) in enumerate(zip(labels, cmaps)):\n",
    "    ax = axes[idx]\n",
    "\n",
    "    df_subset = df_emb.query(f\"label == {idx}\")\n",
    "\n",
    "    ax.hexbin(\n",
    "        df_subset[\"X\"],\n",
    "        df_subset[\"Y\"],\n",
    "        cmap=cmap,\n",
    "        gridsize=20,\n",
    "        linewidths=0\n",
    "    )\n",
    "\n",
    "    ax.set_title(label_name)\n",
    "    ax.set_xticks([])\n",
    "    ax.set_yticks([])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5635a9e0",
   "metadata": {},
   "source": [
    "The plot reveals several intuitive groupings in the emotion space. Negative emotions such as `sadness`, `anger`, and `fear` tend to cluster in nearby regions, with only subtle differences in their distributions. In contrast, `joy` and `love` are clearly separated from these negative clusters and occupy a similar, distinct area, while surprise appears dispersed across the space without a clear cluster. Some separation between emotions is therefore visible, but it is not guaranteed or perfect, since the model was never explicitly trained to distinguish emotion classes and instead acquired this structure only indirectly through its masked language modeling objective."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13bbf392",
   "metadata": {},
   "source": [
    "#### Training a Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae3f5668",
   "metadata": {},
   "source": [
    "The hidden states show some differentiation between emotions, even if clear boundaries are missing for several classes. Next, these representations will be used as features to train a **logistic regression classifier** with Scikit-Learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "284f83e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "lr_clf = LogisticRegression(max_iter=3000)\n",
    "lr_clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7f58b17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate performance\n",
    "lr_clf.score(X_valid, y_valid)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da4fde05",
   "metadata": {},
   "source": [
    "At first glance, the accuracy suggests the model is only slightly better than random guessing, but because the dataset is an imbalanced multiclass problem, this performance is actually substantially stronger than chance. A better way to judge the model is to compare it to a simple baseline."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36de8d84",
   "metadata": {},
   "source": [
    "In Scikit-Learn, the `DummyClassifier` provides such baselines by using trivial strategies, like always predicting the majority class or sampling labels at random, against which the logistic regression can be meaningfully evaluated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43d4ce9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.dummy import DummyClassifier\n",
    "\n",
    "dummy_clf = DummyClassifier(strategy=\"most_frequent\")\n",
    "dummy_clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5012eda2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate performance\n",
    "dummy_clf.score(X_valid, y_valid)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "518800d0",
   "metadata": {},
   "source": [
    "The simple classifier built on top of DistilBERT embeddings clearly outperforms the baseline. To analyze its behavior in more detail, the next step is to inspect the **confusion matrix**, which summarizes how often each true label is predicted as each possible class and thus reveals systematic patterns of errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ba1409b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import ConfusionMatrixDisplay, confusion_matrix\n",
    "\n",
    "def plot_confusion_matrix(y_true, y_preds, labels):\n",
    "\n",
    "    # Compute normalized confusion matrix\n",
    "    cm = confusion_matrix(y_true, y_preds, normalize=\"true\")\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(6, 6))\n",
    "    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=labels)\n",
    "    disp.plot(cmap=\"Blues\", values_format=\".2f\", ax=ax, colorbar=False)\n",
    "    \n",
    "    ax.set_title(\"Normalized Confusion Matrix\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a0e3271",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate predictions and plot\n",
    "y_preds = lr_clf.predict(X_valid)\n",
    "display(plot_confusion_matrix(y_valid, y_preds, labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5837468d",
   "metadata": {},
   "source": [
    "The confusion matrix shows that `anger` and `fear` are most frequently misclassified as `sadness`, consistent with the overlap observed in the embedding visualization. `love` and `surprise` are also often predicted as `joy`, reflecting the close proximity of these emotions in the learned representation space."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f0d0fdd",
   "metadata": {},
   "source": [
    "### Fine-Tuning Transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b7bda31",
   "metadata": {},
   "source": [
    "**Fine-tuning** a transformer end-to-end follows a different strategy from using it purely as a feature extractor. Instead of treating the hidden states as fixed features, they are updated during training along with the rest of the model. This in turn requires the classification head to be fully differentiable, which is why fine-tuning is typically done with a neural network classifier on top of the transformer backbone."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d6b8130",
   "metadata": {},
   "source": [
    "By training the hidden states used as inputs to the classifier, the model is no longer constrained to work with representations that might be suboptimal for the task. Instead, these hidden states are progressively adapted during fine-tuning to reduce the loss, leading to representations that are better aligned with the classification objective and thus improving performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c8f1637",
   "metadata": {},
   "source": [
    "#### Loading a Pretrained Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a14bdbb9",
   "metadata": {},
   "source": [
    "The first ingredient is the same **pretrained** DistilBERT backbone used in the feature-based approach. The only change is that, instead of `AutoModel`, a `AutoModelForSequenceClassification` variant is loaded, which adds a trainable classification head on top of the transformer outputs. This head is configured by specifying the number of target labels (six in this case), which determines the dimensionality of its output layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad7b4dd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSequenceClassification\n",
    "\n",
    "num_labels = 6\n",
    "model = (AutoModelForSequenceClassification\n",
    "         .from_pretrained(model_ckpt, num_labels=num_labels)\n",
    "         .to(device))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fdbe380",
   "metadata": {},
   "source": [
    "#### Performance Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58574408",
   "metadata": {},
   "source": [
    "To track metrics during training, a  `compute_metrics()` function must be defined for the `Trainer`. This function takes an `EvalPrediction` object (which is a named tuple containing predictions and `label_ids`) and returns a dictionary mapping metric names to their corresponding values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c64a04c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score, accuracy_score\n",
    "\n",
    "def compute_metrics(pred):\n",
    "    \n",
    "    # Extract true labels and predicted labels\n",
    "    labels = pred.label_ids\n",
    "    preds = pred.predictions.argmax(axis=-1)\n",
    "    \n",
    "    # Compute metrics\n",
    "    f1 = f1_score(labels, preds, average=\"weighted\")\n",
    "    acc = accuracy_score(labels, preds)\n",
    "    \n",
    "    return {\n",
    "        \"accuracy\": acc,\n",
    "        \"f1\": f1\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "534902df",
   "metadata": {},
   "source": [
    "Before initializing the `Trainer`, there are two remaining preparations:\n",
    "\n",
    "- Log in to a Hugging Face Hub account so the fine-tuned model can be pushed to the Hub and shared with others.\n",
    "\n",
    "- Specify the training hyperparameters (for example, learning rate, batch size, and number of epochs) that will govern the fine-tuning run."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df2a2773",
   "metadata": {},
   "source": [
    "#### Exercise 3: Feature Extraction & Model Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40b28d80",
   "metadata": {},
   "source": [
    "1. Try using different classifiers from scikit-learn instead of logistic regression. Test at least two of the following: `RandomForestClassifier`, `SVC` or `MLPClassifier`. Compare their performance with logistic regression. Which one works best and why?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50a8ea04",
   "metadata": {},
   "source": [
    "2.  The UMAP visualization uses `n_components=2` and `metric=\"cosine\"`. Read the [UMAP documentation](https://umap-learn.readthedocs.io/en/latest/) and experiment with different values for `n_components`, `metric`, and `n_neighbors` parameters. How does this affect the visualization and what insights can you gain?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ae386d1",
   "metadata": {},
   "source": [
    "#### Training a Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4494e2a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "020387e1",
   "metadata": {},
   "source": [
    "A widget will then appear, allowing you to enter either your username and password or an access token with write permissions. Instructions for creating such access tokens are available in the Hub documentation under the user access tokens section."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "transformer_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
