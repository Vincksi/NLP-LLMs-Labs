{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7f4a9f9d",
   "metadata": {},
   "source": [
    "### Tokenization & Embedding for LLMs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6f8546f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"HF_HUB_DISABLE_SYMLINKS_WARNING\"] = \"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a63dc686",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model and tokenizer loaded on cpu\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "\n",
    "# Select device & model\n",
    "device = torch.device(\"cpu\")\n",
    "model_name = \"microsoft/phi-1_5\"\n",
    "\n",
    "# Load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    model_name,\n",
    "    padding_side=\"left\",\n",
    "    truncation_side=\"left\"\n",
    ")\n",
    "\n",
    "# Load model\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    dtype=torch.float32,\n",
    "    device_map={\"\": device},\n",
    "    low_cpu_mem_usage=True,\n",
    "    trust_remote_code=False\n",
    ").eval()\n",
    "\n",
    "print(\"Model and tokenizer loaded on\", device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac523de8",
   "metadata": {},
   "source": [
    "Let‚Äôs try generating some text with our model to see how it reacts to the prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "aabf318d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Write an email apologizing to Sarah for the tragic gardening mishap.Explain how it happened.<|assistant|>\n",
      "\n",
      "Dear Sarah,\n",
      "\n",
      "I hope this email finds you well. I wanted to take a moment to apologize for the unfortunate incident that occurred in your garden. It was truly a heartbreaking sight to see your beautiful flowers and plants destroyed by the strong winds. I understand how much effort and care you put into maintaining your garden, and I am truly sorry for the damage caused.\n",
      "\n",
      "I want to assure you that I will do everything in my power to make it right. I will personally visit\n"
     ]
    }
   ],
   "source": [
    "prompt = (\"Write an email apologizing to Sarah for the tragic gardening mishap.\" \n",
    "          \"Explain how it happened.<|assistant|>\")\n",
    "\n",
    "# Tokenize the input prompt\n",
    "input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids.to(\"cpu\")\n",
    "\n",
    "# Generate the text\n",
    "generation_output = model.generate(\n",
    "  input_ids=input_ids,\n",
    "  max_new_tokens=100\n",
    ")\n",
    "\n",
    "# Print the output\n",
    "print(tokenizer.decode(generation_output[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3742e525",
   "metadata": {},
   "source": [
    "Let‚Äôs take a look at the input and output tokens underlying this text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a8662c85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[16594,   281,  3053, 47401,   284, 10490,   329,   262, 15444, 46072,\n",
      "         29406,   499,    13, 18438,   391,   703,   340,  3022, 29847,    91,\n",
      "           562, 10167,    91,    29]])\n"
     ]
    }
   ],
   "source": [
    "# For inspection, print the input IDs\n",
    "print(input_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f68859a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Write an email apologizing to Sarah for the tragic gardening mishap.Explain how it happened.<|assistant|>\n"
     ]
    }
   ],
   "source": [
    "# Decode and print each input ID sequence\n",
    "for i in input_ids:\n",
    "    print(tokenizer.decode(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d4bec040",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Write\n",
      " an\n",
      " email\n",
      " apologizing\n",
      " to\n",
      " Sarah\n",
      " for\n",
      " the\n",
      " tragic\n",
      " gardening\n",
      " mish\n",
      "ap\n",
      ".\n",
      "Expl\n",
      "ain\n",
      " how\n",
      " it\n",
      " happened\n",
      ".<\n",
      "|\n",
      "ass\n",
      "istant\n",
      "|\n",
      ">\n"
     ]
    }
   ],
   "source": [
    "# Decode and print each input ID in the first batch\n",
    "for i in input_ids[0]:\n",
    "    print(tokenizer.decode(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "18cf56c2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[16594,   281,  3053, 47401,   284, 10490,   329,   262, 15444, 46072,\n",
       "         29406,   499,    13, 18438,   391,   703,   340,  3022, 29847,    91,\n",
       "           562, 10167,    91,    29,   198,   198, 20266, 10490,    11,   198,\n",
       "           198,    40,  2911,   428,  3053,  7228,   345,   880,    13,   314,\n",
       "          2227,   284,  1011,   257,  2589,   284, 16521,   329,   262, 14855,\n",
       "          4519,   326,  5091,   287,   534, 11376,    13,   632,   373,  4988,\n",
       "           257, 37154,  6504,   284,   766,   534,  4950, 12734,   290,  6134,\n",
       "          6572,   416,   262,  1913, 13520,    13,   314,  1833,   703,   881,\n",
       "          3626,   290,  1337,   345,  1234,   656, 10941,   534, 11376,    11,\n",
       "           290,   314,   716,  4988,  7926,   329,   262,  2465,  4073,    13,\n",
       "           198,   198,    40,   765,   284, 19832,   345,   326,   314,   481,\n",
       "           466,  2279,   287,   616,  1176,   284,   787,   340,   826,    13,\n",
       "           314,   481,  7620,  3187]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(generation_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2b379b44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Write\n",
      " an\n"
     ]
    }
   ],
   "source": [
    "# Decode and print specific token IDs\n",
    "print(tokenizer.decode(16594))\n",
    "print(tokenizer.decode(281))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af2ea26b",
   "metadata": {},
   "source": [
    "#### LLM Tokenizers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31cc6f75",
   "metadata": {},
   "source": [
    "We wants to compare how different **LLM tokenizers** split the same input text into tokens, and to implement a small function that performs this comparison for several models on a shared string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cd954d95",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_tokenizers(sentence, tokenizer_names, max_tokens_display=100):\n",
    "    \"\"\"\n",
    "    Compare how different tokenizers split the same text.\n",
    "    Shows token count, token IDs, and the actual tokens (NOT decoded text).\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\"*100)\n",
    "    print(f\"Comparing tokenizers on text:\\n{sentence}\\n\")\n",
    "    print(\"=\"*100 + \"\\n\")\n",
    "\n",
    "    for name in tokenizer_names:\n",
    "        print(f\"Tokenizer: {name}\")\n",
    "\n",
    "        try:\n",
    "            tokenizer = AutoTokenizer.from_pretrained(name)\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to load tokenizer: {e}\\n\")\n",
    "            continue\n",
    "\n",
    "        # Tokenize\n",
    "        enc = tokenizer(sentence, return_tensors=\"pt\")\n",
    "        ids = enc.input_ids[0].tolist()\n",
    "        tokens = tokenizer.convert_ids_to_tokens(ids)\n",
    "\n",
    "        # Truncate long outputs for readability\n",
    "        if len(tokens) > max_tokens_display:\n",
    "            ids_display = ids[:max_tokens_display] + [\"...\"]\n",
    "            tokens_display = tokens[:max_tokens_display] + [\"...\"]\n",
    "        else:\n",
    "            ids_display = ids\n",
    "            tokens_display = tokens\n",
    "\n",
    "        print(f\"Token count: {len(ids)}\")\n",
    "        print(f\"Token IDs  : {ids_display}\")\n",
    "        print(f\"Tokens     : {tokens_display}\")\n",
    "        print(\"\\n\" + \"-\"*100 + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "190463e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "====================================================================================================\n",
      "Comparing tokenizers on text:\n",
      "Yesterday, I emailed Sarah about 3 unexpected ü§ñ updates‚Äînone were actually urgent!\n",
      "\n",
      "====================================================================================================\n",
      "\n",
      "Tokenizer: microsoft/phi-1_5\n",
      "Token count: 18\n",
      "Token IDs  : [28065, 11, 314, 24315, 10490, 546, 513, 10059, 12520, 97, 244, 5992, 960, 23108, 547, 1682, 18039, 0]\n",
      "Tokens     : ['Yesterday', ',', 'ƒ†I', 'ƒ†emailed', 'ƒ†Sarah', 'ƒ†about', 'ƒ†3', 'ƒ†unexpected', 'ƒ†√∞≈Å', '¬§', 'ƒ∏', 'ƒ†updates', '√¢ƒ¢ƒ∂', 'none', 'ƒ†were', 'ƒ†actually', 'ƒ†urgent', '!']\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "Tokenizer: microsoft/Phi-3-mini-4k-instruct\n",
      "Token count: 25\n",
      "Token IDs  : [612, 18358, 29892, 306, 321, 655, 2356, 19235, 1048, 29871, 29941, 15668, 29871, 243, 162, 167, 153, 11217, 30003, 9290, 892, 2869, 5065, 5362, 29991]\n",
      "Tokens     : ['‚ñÅY', 'esterday', ',', '‚ñÅI', '‚ñÅe', 'ma', 'iled', '‚ñÅSarah', '‚ñÅabout', '‚ñÅ', '3', '‚ñÅunexpected', '‚ñÅ', '<0xF0>', '<0x9F>', '<0xA4>', '<0x96>', '‚ñÅupdates', '‚Äî', 'none', '‚ñÅwere', '‚ñÅactually', '‚ñÅur', 'gent', '!']\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "Tokenizer: bert-base-uncased\n",
      "Token count: 19\n",
      "Token IDs  : [101, 7483, 1010, 1045, 10373, 2098, 4532, 2055, 1017, 9223, 100, 14409, 1517, 3904, 2020, 2941, 13661, 999, 102]\n",
      "Tokens     : ['[CLS]', 'yesterday', ',', 'i', 'email', '##ed', 'sarah', 'about', '3', 'unexpected', '[UNK]', 'updates', '‚Äî', 'none', 'were', 'actually', 'urgent', '!', '[SEP]']\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "Tokenizer: google/flan-t5-small\n",
      "Token count: 21\n",
      "Token IDs  : [18566, 6, 27, 3, 15, 19422, 8077, 81, 220, 7544, 3, 2, 3864, 318, 29, 782, 130, 700, 10839, 55, 1]\n",
      "Tokens     : ['‚ñÅYesterday', ',', '‚ñÅI', '‚ñÅ', 'e', 'mailed', '‚ñÅSarah', '‚ñÅabout', '‚ñÅ3', '‚ñÅunexpected', '‚ñÅ', '<unk>', '‚ñÅupdates', '‚Äî', 'n', 'one', '‚ñÅwere', '‚ñÅactually', '‚ñÅurgent', '!', '</s>']\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Example: Compare how different tokenizers handle the same text\n",
    "test_sentence = \"Yesterday, I emailed Sarah about 3 unexpected ü§ñ updates‚Äînone were actually urgent!\"\n",
    "\n",
    "# List of tokenizers to compare\n",
    "tokenizers_to_test = [\n",
    "    \"microsoft/phi-1_5\",\n",
    "    \"microsoft/Phi-3-mini-4k-instruct\",\n",
    "    \"bert-base-uncased\",\n",
    "    \"google/flan-t5-small\",\n",
    "]\n",
    "\n",
    "# Run comparison\n",
    "compare_tokenizers(\n",
    "    test_sentence,\n",
    "    tokenizer_names=tokenizers_to_test,\n",
    "    max_tokens_display=80\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ede9068",
   "metadata": {},
   "source": [
    "These ‚Äúunusual‚Äù tokens in the output, such as `[CLS]`, `[SEP]`, `[UNK]`, `[PAD]`, `[MASK]`, `<s>`, `</s>`, and characters like `ÔøΩ` or `#`, are special tokens that guide how the model processes and interprets text."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49e724e4",
   "metadata": {},
   "source": [
    "**Special tokens overview**\n",
    "- `[CLS]` is a classification token placed at the beginning of the sequence to represent the whole input for tasks like sentence classification.\n",
    "\n",
    "- `[SEP]` is a separator token used to mark boundaries between segments, for example between two sentences in pair-input tasks.\n",
    "\n",
    "- `[UNK]` is an unknown token used when a piece of text cannot be mapped to any known token in the vocabulary.\n",
    "\n",
    "- `[PAD]` is a padding token used to extend shorter sequences so that a batch of inputs all has the same length.\n",
    "\n",
    "- `[MASK]` is a masking token used during masked language modeling, where some tokens are hidden so the model learns to predict them from context.\n",
    "\n",
    "- `<s>` and `</s>` are start and end tokens that mark the beginning and end of a sequence, often used in encoder-decoder or sequence-to-sequence models.\n",
    "\n",
    "- `ƒ†` (used in GPT-style byte-level BPE tokenizers) is a special prefix that encodes a preceding space, indicating that the token starts after a whitespace character.\n",
    "\n",
    "- Symbols like `ÔøΩ` or `#` can indicate problematic or un-decodable characters, or subword fragments, when the tokenizer cannot cleanly map bytes or characters back to readable text."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cf70622",
   "metadata": {},
   "source": [
    "Different tokenization methods exist because they make different trade-offs in speed, vocabulary size, domain coverage, and final model quality.\n",
    "\n",
    "**Why multiple tokenization techniques?**\n",
    "\n",
    "- **Vocabulary size vs efficiency:** Methods like BPE, WordPiece, and SentencePiece slice text into subwords differently, trading off a larger vocabulary for fewer, more meaningful tokens per sentence. Fewer tokens generally mean faster inference and lower compute cost, but overly fragmented words can weaken semantic representation.\n",
    "\n",
    "- **Language and domain needs:** Some tokenizers normalize or lowercase (e.g., uncased BERT) which is fine for many NLP tasks, while others must preserve case, punctuation, and exact forms for code, names, or technical jargon. In specialized domains (medical, legal, code), it is often better to keep domain terms intact instead of breaking them into arbitrary fragments.\n",
    "\n",
    "- **Impact on generation and context:** Since LLMs generate one token at a time, splitting a word into many pieces increases the number of prediction steps, slows generation, and gives more room for errors to accumulate. Inefficient tokenization also burns through the context window faster, reducing how much text the model can consider at once and potentially degrading performance on long inputs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa1280d3",
   "metadata": {},
   "source": [
    "#### Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9aefdd7d",
   "metadata": {},
   "source": [
    "Now that we understand how tokens represent text, we can look at how they are mapped into a high-dimensional vector space. In essence, an LLM converts each token into a numerical vector (an embedding) and then combines and transforms these token vectors to represent and generate text."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "488a825a",
   "metadata": {},
   "source": [
    "The purpose of **embeddings** is to represent text in an appropriate **vector space** so that models can capture meaning and support operations such as similarity search, clustering, and downstream prediction. When a model is downloaded from a model hub, its configuration typically defines an embedding matrix that maps token IDs to dense vectors. At the start of training, this embedding matrix is initialized randomly (like the rest of the model‚Äôs weights), and during training it is updated so that the vectors reflect the structure of the language and the specifics of the task."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4565280d",
   "metadata": {},
   "source": [
    "Let‚Äôs begin by loading a model and its tokenizer so we can inspect the embedding matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9a7e8872",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModel\n",
    "\n",
    "# Load a tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"microsoft/deberta-base\")\n",
    "model = AutoModel.from_pretrained(\"microsoft/deberta-v3-xsmall\")\n",
    "\n",
    "# Tokenize the sentence\n",
    "tokens = tokenizer('Hello world', return_tensors='pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d620a25d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[    1, 31414,   232,     2]]), 'token_type_ids': tensor([[0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1]])}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3faf5dd",
   "metadata": {},
   "source": [
    "**Token IDs (integers)**\n",
    "\n",
    "- These are the discrete indices assigned by the tokenizer‚Äôs vocabulary.\n",
    "\n",
    "- They might look like `[1, 31414, 232, 2]`, where each number corresponds to a specific token, and certain IDs may be reserved for things like padding.\n",
    "\n",
    "- Token IDs can be converted back to text using `tokenizer.decode()` because they directly reference entries in the vocabulary."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8222894",
   "metadata": {},
   "source": [
    "**Embeddings (float vectors)**\n",
    "\n",
    "- These are continuous, high-dimensional vectors learned by the model to capture semantic and syntactic information.\n",
    "\n",
    "- A single token embedding might look like `[-3.4816, 0.0861, -0.1819, ...]` with, for example, 384 dimensions.\n",
    "\n",
    "- Embeddings cannot be directly decoded back to text; instead, each token ID is mapped to its corresponding embedding vector before being fed into the model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46ee0eba",
   "metadata": {},
   "source": [
    "In short, the pipeline is: **Text ‚Üí Token IDs ‚Üí Embeddings ‚Üí Model processing**, with IDs acting as symbolic references and embeddings as the model‚Äôs working numerical representation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d42bc2ad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-3.4816,  0.0861, -0.1819,  ..., -0.0612, -0.3911,  0.3017],\n",
       "         [ 0.1898,  0.3208, -0.2315,  ...,  0.3714,  0.2478,  0.8048],\n",
       "         [ 0.2071,  0.5036, -0.0485,  ...,  1.2175, -0.2292,  0.8582],\n",
       "         [-3.4278,  0.0645, -0.1427,  ...,  0.0658, -0.4367,  0.3834]]],\n",
       "       grad_fn=<NativeLayerNormBackward0>)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Process the tokens\n",
    "output = model(**tokens)[0]\n",
    "display(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3e6d727d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 4, 384])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check the shape of the output\n",
    "output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "560138ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[CLS]Hello world[SEP]'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Decode the TOKEN IDs\n",
    "tokenizer.decode(tokens['input_ids'][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "881d77e6",
   "metadata": {},
   "source": [
    "Here, `output` is a tensor of continuous embedding vectors with shape `[1, 4, 384]`, so it cannot be converted back to text. These values encode semantic information rather than discrete token IDs. In contrast, `tokens[\"input_ids\"] = [1, 31414, 232, 2]` are vocabulary indices that can be decoded back into text, whereas `output = [[-3.48, 0.086, ...], [0.19, 0.32, ...], ...]` represents the corresponding 384‚Äëdimensional embeddings for each token and therefore cannot be directly decoded to text."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "735fc4a2",
   "metadata": {},
   "source": [
    "Now that we understand what embeddings are meant to do, let‚Äôs start playing with word embeddings to see how they work in practice and how they represent text as points in a vector space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "968ab425",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim.downloader as api\n",
    "\n",
    "# Download embeddings (66MB, glove, trained on wikipedia, vector size: 50)\n",
    "model = api.load(\"glove-wiki-gigaword-50\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "5f8d68de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('engineer', 1.0),\n",
       " ('mechanic', 0.7610689401626587),\n",
       " ('technician', 0.7588813900947571),\n",
       " ('engineers', 0.7152684926986694),\n",
       " ('worked', 0.7083118557929993),\n",
       " ('pioneer', 0.7055997848510742),\n",
       " ('retired', 0.6979386806488037),\n",
       " ('chemist', 0.6946015954017639),\n",
       " ('engineering', 0.6913756132125854),\n",
       " ('contractor', 0.6868984699249268),\n",
       " ('builder', 0.6847971677780151)]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# print the 11 most similar words to \"Engineer\"\n",
    "model.most_similar([model['engineer']], topn=11)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32307c25",
   "metadata": {},
   "source": [
    "This technique, often called **word2vec similarity**, is very useful for finding words that are close in meaning to a given word and can also power simple recommendation systems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "eb6a2cba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "import pandas as pd\n",
    "from urllib import request\n",
    "\n",
    "# URLs of the data\n",
    "PLAYLIST_URL = \"https://storage.googleapis.com/maps-premium/dataset/yes_complete/train.txt\"\n",
    "SONGS_URL = \"https://storage.googleapis.com/maps-premium/dataset/yes_complete/song_hash.txt\"\n",
    "\n",
    "# Load the playlist file\n",
    "with request.urlopen(PLAYLIST_URL) as resp:\n",
    "    text = resp.read().decode(\"utf-8\")\n",
    "\n",
    "# Skip the first two metadata lines\n",
    "lines = text.splitlines()[2:]\n",
    "\n",
    "# Build playlists, removing those with only one song\n",
    "playlists = []\n",
    "for line in lines:\n",
    "    line = line.strip()\n",
    "    if not line:\n",
    "        continue\n",
    "    parts = line.split()\n",
    "    if len(parts) > 1:\n",
    "        playlists.append(parts)\n",
    "\n",
    "# Load the song metadata file\n",
    "with request.urlopen(SONGS_URL) as resp:\n",
    "    songs_text = resp.read().decode(\"utf-8\")\n",
    "\n",
    "# Read metadata into a DataFrame\n",
    "songs_buffer = io.StringIO(songs_text)\n",
    "songs_df = pd.read_csv(\n",
    "    songs_buffer,\n",
    "    sep=\"\\t\",\n",
    "    header=None,\n",
    "    names=[\"id\", \"title\", \"artist\"],\n",
    "    dtype={\"id\": str, \"title\": str, \"artist\": str},\n",
    ")\n",
    "\n",
    "# Clean up and set index\n",
    "songs_df = songs_df.dropna(how=\"all\")\n",
    "songs_df = songs_df.set_index(\"id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "063361db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Playlist #1:\n",
      "  ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '20', '21', '22', '23', '24', '25', '26', '27', '28', '29', '30', '31', '32', '33', '34', '35', '36', '37', '38', '39', '40', '41', '2', '42', '43', '44', '45', '46', '47', '48', '20', '49', '8', '50', '51', '52', '53', '54', '55', '56', '57', '25', '58', '59', '60', '61', '62', '3', '63', '64', '65', '66', '46', '47', '67', '2', '48', '68', '69', '70', '57', '50', '71', '72', '53', '73', '25', '74', '59', '20', '46', '75', '76', '77', '59', '20', '43'] \n",
      "\n",
      "Playlist #2:\n",
      "  ['78', '79', '80', '3', '62', '81', '14', '82', '48', '83', '84', '17', '85', '86', '87', '88', '74', '89', '90', '91', '4', '73', '62', '92', '17', '53', '59', '93', '94', '51', '50', '27', '95', '48', '96', '97', '98', '99', '100', '57', '101', '102', '25', '103', '3', '104', '105', '106', '107', '47', '108', '109', '110', '111', '112', '113', '25', '63', '62', '114', '115', '84', '116', '117', '118', '119', '120', '121', '122', '123', '50', '70', '71', '124', '17', '85', '14', '82', '48', '125', '47', '46', '72', '53', '25', '73', '4', '126', '59', '74', '20', '43', '127', '128', '129', '13', '82', '48', '130', '131', '132', '133', '134', '135', '136', '137', '59', '46', '138', '43', '20', '139', '140', '73', '57', '70', '141', '3', '1', '74', '142', '143', '144', '145', '48', '13', '25', '146', '50', '147', '126', '59', '20', '148', '149', '150', '151', '152', '56', '153', '154', '155', '156', '157', '158', '159', '160', '161', '162', '163', '164', '165', '166', '167', '168', '169', '170', '171', '172', '173', '174', '175', '60', '176', '51', '177', '178', '179', '180', '181', '182', '183', '184', '185', '57', '186', '187', '188', '189', '190', '191', '46', '192', '193', '194', '195', '196', '197', '198', '25', '199', '200', '49', '201', '100', '202', '203', '204', '205', '206', '207', '32', '208', '209', '210']\n"
     ]
    }
   ],
   "source": [
    "print('Playlist #1:\\n ', playlists[0], '\\n')\n",
    "print('Playlist #2:\\n ', playlists[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70a7e432",
   "metadata": {},
   "source": [
    "#### Train word2vec model for recommendation song system"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0c7b925",
   "metadata": {},
   "source": [
    "With our playlists and song metadata prepared, we can now train a word2vec model to generate song recommendations from a user‚Äôs playlist. We will use the `Word2Vec` implementation from the `gensim` library.\n",
    "\n",
    "- `vector_size`: controls the dimensionality of the embedding vectors.\n",
    "\n",
    "- `window`: defines how many neighboring items are considered as context.\n",
    "\n",
    "- `negative`: sets the number of negative samples used during training.\n",
    "\n",
    "- `min_count`: specifies the minimum number of occurrences required for a song to be included in the vocabulary.\n",
    "\n",
    "- `workers`: determines how many CPU cores are used for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "ac209876",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: 'gensim.models.word2vec_inner.our_dot_float'\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import Word2Vec\n",
    "\n",
    "# train our Word2Vec model\n",
    "model = Word2Vec(\n",
    "    playlists, \n",
    "    vector_size=32, \n",
    "    window=20, \n",
    "    negative=50, \n",
    "    min_count=1, \n",
    "    workers=4\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "27a4f507",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('3094', 0.997779130935669),\n",
       " ('3167', 0.9968992471694946),\n",
       " ('2849', 0.9965309500694275),\n",
       " ('2704', 0.9964370131492615),\n",
       " ('2976', 0.9958822131156921),\n",
       " ('6624', 0.99538654088974),\n",
       " ('3148', 0.9952204823493958),\n",
       " ('10084', 0.9950636625289917),\n",
       " ('3126', 0.9946957230567932),\n",
       " ('3079', 0.9944549798965454)]"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Ask the model for songs similar to song #2172\n",
    "song_id = 2172\n",
    "model.wv.most_similar(positive=str(song_id))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "5df3492c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "title     Fade To Black\n",
      "artist        Metallica\n",
      "Name: 2172 , dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(songs_df.iloc[2172])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "0c0cf5e7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>title</th>\n",
       "      <th>artist</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3094</td>\n",
       "      <td>Breaking The Law</td>\n",
       "      <td>Judas Priest</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3167</td>\n",
       "      <td>Unchained</td>\n",
       "      <td>Van Halen</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2849</td>\n",
       "      <td>Run To The Hills</td>\n",
       "      <td>Iron Maiden</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2704</td>\n",
       "      <td>Over The Mountain</td>\n",
       "      <td>Ozzy Osbourne</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2976</td>\n",
       "      <td>I Don't Know</td>\n",
       "      <td>Ozzy Osbourne</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      id              title         artist\n",
       "0  3094    Breaking The Law   Judas Priest\n",
       "1  3167           Unchained      Van Halen\n",
       "2  2849    Run To The Hills    Iron Maiden\n",
       "3  2704   Over The Mountain  Ozzy Osbourne\n",
       "4  2976        I Don't Know  Ozzy Osbourne"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def print_recommendations(song_id, topn=5):\n",
    "    similar_ids = [sid for sid, _ in model.wv.most_similar(positive=[str(song_id)], topn=topn)]\n",
    "\n",
    "    # Try direct lookup by index\n",
    "    if all(sid in songs_df.index for sid in similar_ids):\n",
    "        recs = songs_df.loc[similar_ids]\n",
    "    else:\n",
    "        # Otherwise treat as integer positions\n",
    "        positions = [int(s) for s in similar_ids if s.isdigit()]\n",
    "        recs = songs_df.iloc[positions]\n",
    "\n",
    "    return recs.reset_index()\n",
    "\n",
    "display(print_recommendations(2172))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "transformer_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
