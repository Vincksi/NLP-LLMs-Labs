{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Fine Tuning Llama 3.2 on Medical Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "x83idENjudRj"
      },
      "outputs": [],
      "source": [
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Setup & Loading Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "AX9bY6yNCXYo"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from transformers import (\n",
        "    AutoModelForCausalLM,\n",
        "    AutoTokenizer,\n",
        "    TrainingArguments,\n",
        "    Trainer,\n",
        "    DataCollatorForLanguageModeling\n",
        ")\n",
        "\n",
        "from datasets import load_dataset\n",
        "from peft import LoraConfig, get_peft_model, TaskType\n",
        "\n",
        "from pathlib import Path\n",
        "import random\n",
        "import string\n",
        "import json\n",
        "import time\n",
        "import re\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tYCdA8qgCcP3",
        "outputId": "f9a95808-e16a-4f74-d27e-ad7b7c503058"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using device: cuda\n"
          ]
        }
      ],
      "source": [
        "# Check if a CUDA-capable GPU is available\n",
        "if torch.cuda.is_available():\n",
        "    # Use GPU for computations if available\n",
        "    device = torch.device(\"cuda\")\n",
        "else:\n",
        "    # Fall back to CPU if no GPU is available\n",
        "    device = torch.device(\"cpu\")\n",
        "\n",
        "print(f\"Using device: {device}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Load the `meta-llama/Llama-3.2-1B-Instruct` checkpoint from the Hugging Face Hub and initialize both `AutoTokenizer` and `AutoModelForCausalLM` with this model ID."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "U2wjDcW-EEXm"
      },
      "outputs": [],
      "source": [
        "model_name = \"HuggingFaceTB/SmolLM2-1.7B-Instruct\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9UNr9u7LEAfa",
        "outputId": "caff1732-b61e-44bf-ebe7-0b41beca5a7d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model loaded: HuggingFaceTB/SmolLM2-1.7B-Instruct\n"
          ]
        }
      ],
      "source": [
        "# Load the tokenizer associated with the pretrained model\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "\n",
        "# Set the padding token to be the same as the end-of-sequence token\n",
        "# (common practice for causal language models)\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "# Specify that padding should be added on the right side of sequences\n",
        "tokenizer.padding_side = \"right\"\n",
        "\n",
        "# Load the pretrained causal language model\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_name,\n",
        "    dtype=torch.float16,        # Use half precision for reduced memory usage and faster computation\n",
        "    device_map={\"\": device},    # Map the entire model to the selected device (CPU or GPU)\n",
        ")\n",
        "\n",
        "print(f\"Model loaded: {model_name}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### LoRA Configuration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5SILCQ9SF3f4",
        "outputId": "e06d5d3a-ac3f-407b-f9f9-3df7d33ce0aa"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Configuring LoRA...\n",
            "trainable params: 18,087,936 || all params: 1,729,464,320 || trainable%: 1.0459\n"
          ]
        }
      ],
      "source": [
        "print(\"\\nConfiguring LoRA...\")\n",
        "\n",
        "# LoRA hyperparameters (easy to tweak or pass as script arguments)\n",
        "LORA_RANK = 16\n",
        "LORA_ALPHA = 32        # typically = 2 * r or = r\n",
        "LORA_DROPOUT = 0.05    # set to 0.0 for small datasets to avoid underfitting\n",
        "\n",
        "# Target modules for a LLaMA/SmolLM-like architecture\n",
        "# Covers both attention layers and MLP layers\n",
        "TARGET_MODULES = [\n",
        "    \"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",   # attention projections\n",
        "    \"gate_proj\", \"up_proj\", \"down_proj\",      # MLP projections\n",
        "]\n",
        "\n",
        "# Define the LoRA configuration\n",
        "lora_config = LoraConfig(\n",
        "    r=LORA_RANK,                    # Rank of the low-rank adapters\n",
        "    lora_alpha=LORA_ALPHA,          # Scaling factor for LoRA updates\n",
        "    lora_dropout=LORA_DROPOUT,      # Dropout applied to LoRA layers\n",
        "    bias=\"none\",                    # Do not train bias parameters\n",
        "    task_type=TaskType.CAUSAL_LM,   # Task type: causal language modeling\n",
        "    target_modules=TARGET_MODULES,  # Modules where LoRA adapters are injected\n",
        ")\n",
        "\n",
        "# Wrap the base model with LoRA adapters\n",
        "model = get_peft_model(model, lora_config)\n",
        "\n",
        "model.print_trainable_parameters()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Loading Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Load the dataset, apply the formatting function to it, and restrict the resulting data to 500 formatted examples for training."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w9-Ri-jVGNiQ"
      },
      "outputs": [],
      "source": [
        "def format_prompt_medmcqa(example):\n",
        "    \"\"\"\n",
        "    Format a MedMCQA example into a chat-style prompt\n",
        "    compatible with SmolLM2-360M-Instruct.\n",
        "    \"\"\"\n",
        "\n",
        "    # Extract MedMCQA fields safely (fallback to empty strings)\n",
        "    question = example.get(\"question\", \"\")\n",
        "    opa = example.get(\"opa\", \"\")\n",
        "    opb = example.get(\"opb\", \"\")\n",
        "    opc = example.get(\"opc\", \"\")\n",
        "    opd = example.get(\"opd\", \"\")\n",
        "    cop = example.get(\"cop\", \"\")  # Correct option: 'a','b','c','d' or 0â€“3 (ClassLabel)\n",
        "\n",
        "    # Map answer label/index to the actual answer text\n",
        "    option_map = {\n",
        "        \"a\": opa,\n",
        "        \"b\": opb,\n",
        "        \"c\": opc,\n",
        "        \"d\": opd,\n",
        "        0: opa,\n",
        "        1: opb,\n",
        "        2: opc,\n",
        "        3: opd,\n",
        "    }\n",
        "    answer = option_map.get(cop, \"\")\n",
        "\n",
        "    # Basic filtering to remove invalid or low-quality examples\n",
        "    if not question or len(question) < 10:\n",
        "        return None\n",
        "    if not answer or len(answer) < 2:\n",
        "        return None\n",
        "\n",
        "    # Format the multiple-choice question with all options\n",
        "    mcq_text = (\n",
        "        f\"{question}\\n\"\n",
        "        f\"A. {opa}\\n\"\n",
        "        f\"B. {opb}\\n\"\n",
        "        f\"C. {opc}\\n\"\n",
        "        f\"D. {opd}\"\n",
        "    )\n",
        "\n",
        "    # SmolLM2 chat-style prompt template\n",
        "    text = (\n",
        "        \"<|im_start|>system\\n\"\n",
        "        \"You are a helpful AI assistant.<|im_end|>\\n\"\n",
        "        \"<|im_start|>user\\n\"\n",
        "        f\"{mcq_text}<|im_end|>\\n\"\n",
        "        \"<|im_start|>assistant\\n\"\n",
        "        f\"The answer is: {answer}<|im_end|>\"\n",
        "    )\n",
        "\n",
        "    # Return formatted text in a dict (compatible with HF datasets mapping)\n",
        "    return {\"text\": text}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zx1T8yRLGOxY",
        "outputId": "7b1ddf97-789c-4876-a986-c6ef5640451e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Dataset({\n",
            "    features: ['text'],\n",
            "    num_rows: 500\n",
            "})\n"
          ]
        }
      ],
      "source": [
        "# 1. Load the MedMCQA dataset\n",
        "# The dataset contains train / validation / test splits\n",
        "dataset = load_dataset(\"openlifescienceai/medmcqa\")\n",
        "raw_train = dataset[\"train\"]\n",
        "\n",
        "# 2. Apply prompt formatting to each training example\n",
        "# This converts raw MedMCQA samples into chat-style prompts\n",
        "formatted = raw_train.map(\n",
        "    format_prompt_medmcqa,\n",
        "    remove_columns=raw_train.column_names,  # Keep only the formatted text\n",
        ")\n",
        "\n",
        "# 3. Filter out invalid examples (where formatting returned None)\n",
        "formatted = formatted.filter(lambda x: x[\"text\"] is not None)\n",
        "\n",
        "# 4. Keep only a subset of examples (e.g., for quick experiments or debugging)\n",
        "train_dataset = formatted.select(range(min(500, len(formatted))))\n",
        "\n",
        "print(train_dataset)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Training & Fine Tuning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uwcvHGQrIL_f"
      },
      "outputs": [],
      "source": [
        "def tokenize_function(examples):\n",
        "    \"\"\"\n",
        "    Tokenize formatted text prompts for causal language model training.\n",
        "    The labels are set to be the same as the input IDs (standard LM objective).\n",
        "    \"\"\"\n",
        "\n",
        "    # Tokenize the input text\n",
        "    tokenized = tokenizer(\n",
        "        examples[\"text\"],          # List of formatted prompts\n",
        "        padding=\"max_length\",      # Pad all sequences to the same length\n",
        "        truncation=True,           # Truncate sequences longer than max_length\n",
        "        max_length=512,            # Maximum sequence length\n",
        "        return_tensors=\"pt\"        # Return PyTorch tensors\n",
        "    )\n",
        "\n",
        "    # For causal language modeling, labels are identical to input_ids\n",
        "    tokenized[\"labels\"] = tokenized[\"input_ids\"].clone()\n",
        "\n",
        "    return tokenized"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136,
          "referenced_widgets": [
            "7b356cfd70cb45fcb7b7dd32c46e4c0d",
            "4caee023b2394d3f9c0ac6f8570e8e59",
            "e3dd67f3d525454db3b766fe10e94243",
            "eb53e4d4ec4f4a71a6f3876d39448780",
            "fdcc5276ad9242eda8d19fbd07a02511",
            "3e9a6c10c163477aa189825c16b214a1",
            "4e1414f19bd2452ead1559c4c2835b28",
            "74e23ea62c764f9cbaddd9e3ea6ce210",
            "075a43deb7fd49d18d6a7cf27d0ff6ce",
            "57865505e22040a6afd2c9968fa274f8",
            "a732e1b91f9a4046b5cdb84f66d75ac6"
          ]
        },
        "id": "yt9X3Nq4Ic2p",
        "outputId": "15df0792-22ce-48e6-8f4b-c122700b242b"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "7b356cfd70cb45fcb7b7dd32c46e4c0d",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Map:   0%|          | 0/500 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Dataset({\n",
            "    features: ['input_ids', 'attention_mask', 'labels'],\n",
            "    num_rows: 500\n",
            "})\n",
            "{'input_ids': torch.Size([512]), 'attention_mask': torch.Size([512]), 'labels': torch.Size([512])}\n"
          ]
        }
      ],
      "source": [
        "# Apply tokenization to the training dataset\n",
        "# batched=True means tokenize_function receives a batch (list) of texts\n",
        "tokenized_train = train_dataset.map(\n",
        "    tokenize_function,\n",
        "    batched=True,\n",
        "    remove_columns=[\"text\"],  # Remove raw text, keep only model inputs\n",
        ")\n",
        "\n",
        "# Ensure outputs are returned as PyTorch tensors when accessed\n",
        "tokenized_train.set_format(\n",
        "    type=\"torch\",\n",
        "    columns=[\"input_ids\", \"attention_mask\", \"labels\"],\n",
        ")\n",
        "\n",
        "# Print dataset structure and metadata\n",
        "print(tokenized_train)\n",
        "\n",
        "# Print the shape of each tensor for the first example (sanity check)\n",
        "print({k: v.shape for k, v in tokenized_train[0].items()})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VK_ntv32Jdez"
      },
      "outputs": [],
      "source": [
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./results\",              # Directory to save model checkpoints and outputs\n",
        "    num_train_epochs=3,                  # Number of full training epochs\n",
        "    per_device_train_batch_size=1,       # Batch size per GPU/CPU\n",
        "    gradient_accumulation_steps=4,       # Accumulate gradients to simulate a larger batch size\n",
        "    learning_rate=2e-4,                  # Initial learning rate\n",
        "    warmup_steps=10,                     # Number of warmup steps for the learning rate scheduler\n",
        "\n",
        "    logging_steps=10,                    # Log training metrics every N steps\n",
        "    save_steps=100,                      # Save a checkpoint every N steps\n",
        "    save_total_limit=2,                  # Keep only the last 2 checkpoints to save disk space\n",
        "\n",
        "    fp16=False,                          # Disable mixed precision (enable if training on GPU with FP16 support)\n",
        "    logging_dir=\"./logs\",                # Directory for TensorBoard logs\n",
        "    report_to=\"none\"                     # Disable external experiment trackers (e.g., WandB)\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "jG2k1uY9Jl_a",
        "outputId": "21ba469b-03ce-4d21-d074-657c4001d7b7"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='375' max='375' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [375/375 09:42, Epoch 3/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>10</td>\n",
              "      <td>2.221800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>20</td>\n",
              "      <td>1.571100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>30</td>\n",
              "      <td>1.427500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>40</td>\n",
              "      <td>1.383800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>50</td>\n",
              "      <td>1.350800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>60</td>\n",
              "      <td>1.412600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>70</td>\n",
              "      <td>1.396300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>80</td>\n",
              "      <td>1.316000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>90</td>\n",
              "      <td>1.296500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>100</td>\n",
              "      <td>1.226000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>110</td>\n",
              "      <td>1.240300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>120</td>\n",
              "      <td>1.355900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>130</td>\n",
              "      <td>1.255200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>140</td>\n",
              "      <td>1.223200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>150</td>\n",
              "      <td>1.111900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>160</td>\n",
              "      <td>1.187000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>170</td>\n",
              "      <td>1.149500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>180</td>\n",
              "      <td>1.127900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>190</td>\n",
              "      <td>1.189500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>200</td>\n",
              "      <td>1.068500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>210</td>\n",
              "      <td>1.236300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>220</td>\n",
              "      <td>1.218500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>230</td>\n",
              "      <td>1.227000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>240</td>\n",
              "      <td>1.207900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>250</td>\n",
              "      <td>1.094600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>260</td>\n",
              "      <td>1.034800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>270</td>\n",
              "      <td>0.881200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>280</td>\n",
              "      <td>0.890500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>290</td>\n",
              "      <td>0.947300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>300</td>\n",
              "      <td>0.951700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>310</td>\n",
              "      <td>0.928000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>320</td>\n",
              "      <td>1.063300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>330</td>\n",
              "      <td>0.951500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>340</td>\n",
              "      <td>0.920100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>350</td>\n",
              "      <td>0.966400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>360</td>\n",
              "      <td>1.004000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>370</td>\n",
              "      <td>1.023100</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "TrainOutput(global_step=375, training_loss=1.1862285041809082, metrics={'train_runtime': 584.0308, 'train_samples_per_second': 2.568, 'train_steps_per_second': 0.642, 'total_flos': 7505515118592000.0, 'train_loss': 1.1862285041809082, 'epoch': 3.0})"
            ]
          },
          "execution_count": 35,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Data collator for causal language modeling\n",
        "# Handles dynamic padding and batch preparation during training\n",
        "data_collator = DataCollatorForLanguageModeling(\n",
        "    tokenizer=tokenizer,\n",
        "    mlm=False               # Disable masked language modeling (use causal LM objective)\n",
        ")\n",
        "\n",
        "# Initialize the Hugging Face Trainer\n",
        "trainer = Trainer(\n",
        "    model=model,            # LoRA-adapted language model\n",
        "    args=training_args,     # Training configuration\n",
        "    train_dataset=tokenized_train,  # Tokenized training dataset\n",
        "    data_collator=data_collator,     # Data collator for batching\n",
        "    tokenizer=tokenizer,    # Tokenizer (used for padding and decoding)\n",
        ")\n",
        "\n",
        "# Start the training process\n",
        "trainer.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eMznssY5zhYW",
        "outputId": "4b4e151b-9f48-4ddd-ba2a-1a6e5783c03d"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "('./llama3_medical_lora/tokenizer_config.json',\n",
              " './llama3_medical_lora/special_tokens_map.json',\n",
              " './llama3_medical_lora/chat_template.jinja',\n",
              " './llama3_medical_lora/vocab.json',\n",
              " './llama3_medical_lora/merges.txt',\n",
              " './llama3_medical_lora/added_tokens.json',\n",
              " './llama3_medical_lora/tokenizer.json')"
            ]
          },
          "execution_count": 36,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model.save_pretrained(\"./llama3_medical_lora\")\n",
        "tokenizer.save_pretrained(\"./llama3_medical_lora\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Evaluation & Performance"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C80_GA7ozlp-",
        "outputId": "0ec83c89-0999-435a-9b26-26d76270dff4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total dataset size: 182822\n",
            "Training set indices: 0 to 999\n",
            "Test set indices: 1000 to 182821\n",
            "Selected 20 test examples\n",
            "First 5 sampled indices in test split: [167621, 29184, 6556, 72097, 64196]\n"
          ]
        }
      ],
      "source": [
        "train = dataset[\"train\"]\n",
        "\n",
        "# Parameters\n",
        "train_cutoff = 1000\n",
        "n_eval_examples = 20\n",
        "seed = 42\n",
        "\n",
        "# Explicitly define dataset splits\n",
        "# Training set: indices [0, train_cutoff - 1]\n",
        "# Test set: indices [train_cutoff, end]\n",
        "train_set = train.select(range(0, min(train_cutoff, len(train))))\n",
        "test_set = train.select(range(train_cutoff, len(train)))\n",
        "\n",
        "# Reproducibly sample examples from the test set\n",
        "rng = random.Random(seed)\n",
        "selected_indices = rng.sample(range(len(test_set)), n_eval_examples)\n",
        "\n",
        "# Create a small evaluation subset\n",
        "eval_subset = test_set.select(selected_indices)\n",
        "\n",
        "# Print dataset statistics and sanity checks\n",
        "print(f\"Total dataset size: {len(train)}\")\n",
        "print(f\"Training set indices: 0 to {len(train_set) - 1}\")\n",
        "print(f\"Test set indices: {train_cutoff} to {len(train) - 1}\")\n",
        "print(f\"Selected {len(selected_indices)} test examples\")\n",
        "print(f\"First 5 sampled indices in test split: {selected_indices[:5]}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c_N_FOid0Ivu"
      },
      "outputs": [],
      "source": [
        "def build_medmcqa_prompt(example):\n",
        "    \"\"\"\n",
        "    Construct a simple multiple-choice question (MCQ) text\n",
        "    from a MedMCQA dataset example.\n",
        "    \"\"\"\n",
        "\n",
        "    # Extract question and answer options from the example\n",
        "    question = example[\"question\"]\n",
        "    opa = example[\"opa\"]\n",
        "    opb = example[\"opb\"]\n",
        "    opc = example[\"opc\"]\n",
        "    opd = example[\"opd\"]\n",
        "\n",
        "    # Format the MCQ as a string with options labeled A-D\n",
        "    mcq_text = (\n",
        "        f\"{question}\\n\"\n",
        "        f\"A. {opa}\\n\"\n",
        "        f\"B. {opb}\\n\"\n",
        "        f\"C. {opc}\\n\"\n",
        "        f\"D. {opd}\"\n",
        "    )\n",
        "\n",
        "    return mcq_text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-BkfCGVmz0bb"
      },
      "outputs": [],
      "source": [
        "def get_prediction_from_example(example, max_tokens=50):\n",
        "    \"\"\"\n",
        "    Generate a model prediction for a single MedMCQA example.\n",
        "\n",
        "    Args:\n",
        "        example (dict): A dataset example containing 'question' and options.\n",
        "        max_tokens (int): Maximum number of tokens to generate.\n",
        "\n",
        "    Returns:\n",
        "        str: The model's generated answer (cleaned of special tokens).\n",
        "    \"\"\"\n",
        "\n",
        "    # 1. Build the multiple-choice question (MCQ) text for the \"user\"\n",
        "    user_content = build_medmcqa_prompt(example)\n",
        "\n",
        "    # 2. Wrap the MCQ in the same chat-style template used for fine-tuning\n",
        "    prompt = (\n",
        "        \"<|begin_of_text|>\"\n",
        "        \"<|start_header_id|>system<|end_header_id|>\\n\\n\"\n",
        "        \"You are a helpful AI medical assistant.\\n\"\n",
        "        \"<|eot_id|>\"\n",
        "        \"<|start_header_id|>user<|end_header_id|>\\n\\n\"\n",
        "        f\"{user_content}\"\n",
        "        \"<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n\"\n",
        "    )\n",
        "\n",
        "    # Tokenize the prompt and move tensors to the correct device (CPU/GPU)\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
        "\n",
        "    # Generate predictions without computing gradients (inference mode)\n",
        "    with torch.no_grad():\n",
        "        outputs = model.generate(\n",
        "            **inputs,\n",
        "            max_new_tokens=max_tokens,  # Limit output length\n",
        "            temperature=0.3,            # Low temperature for more deterministic output\n",
        "            top_p=0.9,                  # Nucleus sampling for diversity\n",
        "            do_sample=True,             # Enable sampling\n",
        "            pad_token_id=tokenizer.eos_token_id,  # Ensure proper padding\n",
        "        )\n",
        "\n",
        "    # Decode the generated token IDs back to text (include special tokens)\n",
        "    full_text = tokenizer.decode(outputs[0], skip_special_tokens=False)\n",
        "\n",
        "    # 3. Extract only the newly generated part (remove the prompt)\n",
        "    generated = full_text[len(prompt):].strip()\n",
        "\n",
        "    # Optional: clean up any leftover special tokens\n",
        "    for tok in [\"<|eot_id|>\", \"<|end_of_text|>\"]:\n",
        "        generated = generated.split(tok)[0].strip()\n",
        "\n",
        "    return generated"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6XE2hfGA0LMh"
      },
      "outputs": [],
      "source": [
        "# Set of common stop words to ignore when computing partial similarity\n",
        "STOP_WORDS = {\n",
        "    \"the\", \"a\", \"an\", \"is\", \"are\", \"was\", \"were\", \"of\", \"in\", \"to\", \"for\",\n",
        "    \"with\", \"on\", \"at\", \"and\", \"or\", \"by\", \"from\"\n",
        "}\n",
        "\n",
        "def _normalize(text: str) -> str:\n",
        "    \"\"\"\n",
        "    Normalize text for comparison:\n",
        "    - Convert to lowercase\n",
        "    - Remove punctuation\n",
        "    - Collapse multiple spaces and strip leading/trailing spaces\n",
        "    \"\"\"\n",
        "    # Convert to lowercase\n",
        "    text = text.lower()\n",
        "    # Remove punctuation\n",
        "    text = text.translate(str.maketrans(\"\", \"\", string.punctuation))\n",
        "    # Normalize spaces\n",
        "    text = re.sub(r\"\\s+\", \" \", text).strip()\n",
        "    return text\n",
        "\n",
        "def check_accuracy(prediction: str, ground_truth: str, partial_threshold: float = 0.7):\n",
        "    \"\"\"\n",
        "    Compare a model prediction against the ground truth.\n",
        "\n",
        "    Returns:\n",
        "        (bool, str): Tuple indicating if it's a match and the match type:\n",
        "                     - \"exact\" for exact match\n",
        "                     - \"partial\" for token-level similarity above threshold\n",
        "                     - \"no_match\" if no significant overlap\n",
        "    \"\"\"\n",
        "\n",
        "    # Normalize both prediction and ground truth\n",
        "    pred_norm = _normalize(prediction)\n",
        "    truth_norm = _normalize(ground_truth)\n",
        "\n",
        "    # 1. Exact match (after normalization)\n",
        "    if truth_norm and truth_norm in pred_norm:\n",
        "        return True, \"exact\"\n",
        "\n",
        "    # 2. Partial similarity on \"informative\" tokens (exclude stop words)\n",
        "    truth_tokens = [w for w in truth_norm.split() if w not in STOP_WORDS]\n",
        "    pred_tokens = [w for w in pred_norm.split() if w not in STOP_WORDS]\n",
        "\n",
        "    # If either has no informative tokens, consider it a non-match\n",
        "    if not truth_tokens or not pred_tokens:\n",
        "        return False, \"no_match\"\n",
        "\n",
        "    # Convert tokens to sets for Jaccard similarity\n",
        "    truth_set = set(truth_tokens)\n",
        "    pred_set = set(pred_tokens)\n",
        "\n",
        "    # Compute Jaccard similarity: intersection / union\n",
        "    intersection = len(truth_set & pred_set)\n",
        "    union = len(truth_set | pred_set)\n",
        "\n",
        "    if union == 0:\n",
        "        return False, \"no_match\"\n",
        "\n",
        "    jaccard = intersection / union\n",
        "\n",
        "    # Consider it a partial match if similarity exceeds threshold\n",
        "    if jaccard >= partial_threshold:\n",
        "        return True, \"partial\"\n",
        "\n",
        "    # Otherwise, no match\n",
        "    return False, \"no_match\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X8KryY3S0VLH",
        "outputId": "54778773-2c0e-4100-d010-e09b58210bd3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "TEST 1/20\n",
            "Question: Which of the following is found in the respiratory zone of the lung?...\n",
            "Options: A) Goblet cells | B) Main bronchi | C) Mucous cells | D) Type I epithelial cells\n",
            "Ground Truth (3): Type I epithelial cells\n",
            "Prediction: 1. The answer is: Mucous cells...\n",
            "INCORRECT\n",
            "Running accuracy: 0.0% (0/1 ; exact=0, partial=0)\n",
            "\n",
            "TEST 2/20\n",
            "Question: Which of the following does not occur in starvation?...\n",
            "Options: A) Hypoglycemia | B) Hypercholesterolemia | C) Lipolyiss | D) Ketoacidosis\n",
            "Ground Truth (1): Hypercholesterolemia\n",
            "Prediction: 1. The answer is: Ketoacidosis...\n",
            "INCORRECT\n",
            "Running accuracy: 0.0% (0/2 ; exact=0, partial=0)\n",
            "\n",
            "TEST 3/20\n",
            "Question: A 20 month old female child is brought for routine check-up. Complete blood count (CBC) shows modera...\n",
            "Options: A) Corticosteroid administration | B) Multivitamin administration | C) Watch and wait strategy | D) Antibiotics to prevent infecti\n",
            "Ground Truth (2): Watch and wait strategy\n",
            "Prediction: 1. The answer is: Watch and wait strategy...\n",
            "CORRECT (exact)\n",
            "Running accuracy: 33.3% (1/3 ; exact=1, partial=0)\n",
            "\n",
            "TEST 4/20\n",
            "Question: Urgent treatment of procainamide toxicity is:...\n",
            "Options: A) Calcium chelation | B) KCI | C) Nitroprusside | D) Sodium lactate\n",
            "Ground Truth (3): Sodium lactate\n",
            "Prediction: 1. All are true about procainamide toxicity except:\n",
            "A. Calcium chelation\n",
            "B. KCI\n",
            "C. Nitroprusside\n",
            "D. ...\n",
            "CORRECT (exact)\n",
            "Running accuracy: 50.0% (2/4 ; exact=2, partial=0)\n",
            "\n",
            "TEST 5/20\n",
            "Question: Which of the following agents is not used in the treatment of Diabetic Macular Edema Retinopathy-...\n",
            "Options: A) Ruboxistaurim | B) Pyridazinones | C) Benfotiamine | D) Tamoxifen\n",
            "Ground Truth (3): Tamoxifen\n",
            "Prediction: 3. The answer is: Ruboxistaurim...\n",
            "INCORRECT\n",
            "Running accuracy: 40.0% (2/5 ; exact=2, partial=0)\n",
            "\n",
            "TEST 6/20\n",
            "Question: Mutation in alpha 5 chain of collagen 4, the diagonis is -...\n",
            "Options: A) Alpo's syndrome | B) Thin membrane disease | C) Nodular glomerulosclerosis | D) Good pasture syndrome\n",
            "Ground Truth (0): Alpo's syndrome\n",
            "Prediction: 3. Which of the following is the most common cause of sudden death in a young athlete?\n",
            "A. Anemia\n",
            "B. ...\n",
            "INCORRECT\n",
            "Running accuracy: 33.3% (2/6 ; exact=2, partial=0)\n",
            "\n",
            "TEST 7/20\n",
            "Question: Which of the following is not done for diagnosis of parotid tumor...\n",
            "Options: A) MRI | B) CT | C) Open surgical biopsy | D) FNAC\n",
            "Ground Truth (2): Open surgical biopsy\n",
            "Prediction: 1. The answer is: MRI...\n",
            "INCORRECT\n",
            "Running accuracy: 28.6% (2/7 ; exact=2, partial=0)\n",
            "\n",
            "TEST 8/20\n",
            "Question: Congenital Infection affecting the fetus with minimal teratogenic risk is:...\n",
            "Options: A) HIV | B) Rubella | C) Varicella | D) CMV\n",
            "Ground Truth (0): HIV\n",
            "Prediction: 1. The answer is: Varicella...\n",
            "INCORRECT\n",
            "Running accuracy: 25.0% (2/8 ; exact=2, partial=0)\n",
            "\n",
            "TEST 9/20\n",
            "Question: Breast carcinoma is associated with all except:...\n",
            "Options: A) BRCA1 | B) BRCA2 | C) TP53 | D) ATR\n",
            "Ground Truth (3): ATR\n",
            "Prediction: 1. The answer is: TP53...\n",
            "INCORRECT\n",
            "Running accuracy: 22.2% (2/9 ; exact=2, partial=0)\n",
            "\n",
            "TEST 10/20\n",
            "Question: For the following statements, select the most likely type of Hodgkin disease (HD).This variant has a...\n",
            "Options: A) lymphocyte-predominant Hodgkin | B) nodular-sclerosing HD | C) mixed-cellularity HD | D) lymphocyte-depleted HD, reticu\n",
            "Ground Truth (0): lymphocyte-predominant Hodgkin disease (HD)\n",
            "Prediction: 1. The answer is: lymphocyte-predominant HD...\n",
            "INCORRECT\n",
            "Running accuracy: 20.0% (2/10 ; exact=2, partial=0)\n",
            "\n",
            "TEST 11/20\n",
            "Question: A 24 year old male presented with retroperitoneal left necrotic mass near the hilum of kidney which ...\n",
            "Options: A) Metastatic transitional cell c | B) Metastatic melanoma | C) Metastatic germ cell tumour | D) Lymphoma\n",
            "Ground Truth (2): Metastatic germ cell tumour\n",
            "Prediction: 3rd answer is: Metastatic melanoma...\n",
            "INCORRECT\n",
            "Running accuracy: 18.2% (2/11 ; exact=2, partial=0)\n",
            "\n",
            "TEST 12/20\n",
            "Question: Polycythemia is seen with which tumor -...\n",
            "Options: A) Renal cell carcinoma | B) Endometrial carcinoma | C) Lung carcinoma | D) Fibrosarcoma\n",
            "Ground Truth (0): Renal cell carcinoma\n",
            "Prediction: 1. The answer is: Endometrial carcinoma...\n",
            "INCORRECT\n",
            "Running accuracy: 16.7% (2/12 ; exact=2, partial=0)\n",
            "\n",
            "TEST 13/20\n",
            "Question: Incidence of which lymphoma is MORE common in females?...\n",
            "Options: A) Mantle cell lymphoma | B) Follicular lymphoma | C) Burkitt's lymphoma | D) Diffuse large B cell lymphoma\n",
            "Ground Truth (1): Follicular lymphoma\n",
            "Prediction: 1. The answer is: Mantle cell lymphoma...\n",
            "INCORRECT\n",
            "Running accuracy: 15.4% (2/13 ; exact=2, partial=0)\n",
            "\n",
            "TEST 14/20\n",
            "Question: A 29-year-old female diagnosed with AIDS has been suffering from a progressive blurring of vision in...\n",
            "Options: A) Acyclovir | B) Amantadine | C) Flucytosine | D) Ganciclovir\n",
            "Ground Truth (3): Ganciclovir\n",
            "Prediction: 1. Answer: Ganciclovir\n",
            "\n",
            "Ganciclovir is a nucleotide analogue of guanine that is incorporated into DN...\n",
            "CORRECT (exact)\n",
            "Running accuracy: 21.4% (3/14 ; exact=3, partial=0)\n",
            "\n",
            "TEST 15/20\n",
            "Question: Death of Poliomyelitis is due to -...\n",
            "Options: A) Infection | B) Neurogenic shock | C) Cardiac failur | D) Respiratory paralysis\n",
            "Ground Truth (3): Respiratory paralysis\n",
            "Prediction: 1. The answer is: Respiratory paralysis...\n",
            "CORRECT (exact)\n",
            "Running accuracy: 26.7% (4/15 ; exact=4, partial=0)\n",
            "\n",
            "TEST 16/20\n",
            "Question: Xenon anesthesia all are true except...\n",
            "Options: A) Slow induction and recovery | B) Non explosive | C) Minimal cardiovascular side ef | D) Low blood solubility\n",
            "Ground Truth (0): Slow induction and recovery\n",
            "Prediction: 3. The answer is: Low blood solubility...\n",
            "INCORRECT\n",
            "Running accuracy: 25.0% (4/16 ; exact=4, partial=0)\n",
            "\n",
            "TEST 17/20\n",
            "Question: Thyroid hormones in blood is transpoed by:...\n",
            "Options: A) Albumin | B) Globulin | C) Prealbumin | D) All\n",
            "Ground Truth (3): All\n",
            "Prediction: 1. Globulin\n",
            "2. Albumin\n",
            "3. Prealbumin\n",
            "4. All of the above...\n",
            "CORRECT (exact)\n",
            "Running accuracy: 29.4% (5/17 ; exact=5, partial=0)\n",
            "\n",
            "TEST 18/20\n",
            "Question: Kernohan's notch is seen in the following organ injury:...\n",
            "Options: A) Brain | B) Lungs | C) Right lobe of the liver | D) Left lobe of the liver\n",
            "Ground Truth (0): Brain\n",
            "Prediction: 1. Kernohan's notch is seen in the following organ injury:\n",
            "A. Brain\n",
            "B. Lungs\n",
            "C. Right lobe of the li...\n",
            "CORRECT (exact)\n",
            "Running accuracy: 33.3% (6/18 ; exact=6, partial=0)\n",
            "\n",
            "TEST 19/20\n",
            "Question: True about bipolar disorder type II is-...\n",
            "Options: A) Recurrent depression | B) Recurrent mania | C) Repetitive depression & mania | D) Repetitive depression & hypoma\n",
            "Ground Truth (3): Repetitive depression & hypomania\n",
            "Prediction: 1. The answer is: Repetitive depression & mania...\n",
            "INCORRECT\n",
            "Running accuracy: 31.6% (6/19 ; exact=6, partial=0)\n",
            "\n",
            "TEST 20/20\n",
            "Question: Lynch Howah surgery is for:...\n",
            "Options: A) Nasal septal perforation | B) Sinonasal tumours | C) Acoustic neuroma | D) Otosclerosis\n",
            "Ground Truth (1): Sinonasal tumours\n",
            "Prediction: 1. The answer is: Nasal septal perforation...\n",
            "INCORRECT\n",
            "Running accuracy: 30.0% (6/20 ; exact=6, partial=0)\n",
            "\n",
            "Finished 20 examples in 66.8 seconds.\n",
            "Final accuracy: 30.0% (exact=6, partial=0)\n"
          ]
        }
      ],
      "source": [
        "results = []\n",
        "correct_exact = 0\n",
        "correct_partial = 0\n",
        "start_time = time.time()\n",
        "\n",
        "n_total = len(selected_indices)  # Total number of evaluation examples\n",
        "\n",
        "# Iterate over the sampled test examples\n",
        "for i, idx in enumerate(selected_indices, 1):\n",
        "    example = test_set[idx]\n",
        "\n",
        "    # Extract MedMCQA fields\n",
        "    question = example[\"question\"]\n",
        "    opa = example[\"opa\"]\n",
        "    opb = example[\"opb\"]\n",
        "    opc = example[\"opc\"]\n",
        "    opd = example[\"opd\"]\n",
        "    cop = example[\"cop\"]  # Correct option: 'a'/'b'/'c'/'d'\n",
        "\n",
        "    # Map the correct option to its text\n",
        "    option_map = {\"a\": opa, \"b\": opb, \"c\": opc, \"d\": opd, 0: opa, 1: opb, 2: opc, 3: opd}\n",
        "    ground_truth = option_map.get(cop, \"\")\n",
        "\n",
        "    # Construct the MCQ text seen by the user (matching fine-tuning format)\n",
        "    user_question = (\n",
        "        f\"{question}\\n\"\n",
        "        f\"A. {opa}\\n\"\n",
        "        f\"B. {opb}\\n\"\n",
        "        f\"C. {opc}\\n\"\n",
        "        f\"D. {opd}\"\n",
        "    )\n",
        "\n",
        "    # Print example info for debugging / tracking\n",
        "    print(f\"\\nTEST {i}/{n_total}\")\n",
        "    print(f\"Question: {question[:100]}...\")\n",
        "    print(f\"Options: A) {opa[:30]} | B) {opb[:30]} | C) {opc[:30]} | D) {opd[:30]}\")\n",
        "    print(f\"Ground Truth ({cop}): {ground_truth}\")\n",
        "\n",
        "    # Call the model to get a prediction (chat-style prompt)\n",
        "    prediction = get_prediction_from_example(example)\n",
        "    print(f\"Prediction: {prediction[:100]}...\")\n",
        "\n",
        "    # Evaluate the prediction against the ground truth\n",
        "    correct, match_type = check_accuracy(prediction, ground_truth)\n",
        "\n",
        "    # Update counters for exact or partial matches\n",
        "    if correct:\n",
        "        if match_type == \"exact\":\n",
        "            correct_exact += 1\n",
        "        else:\n",
        "            correct_partial += 1\n",
        "        print(f\"CORRECT ({match_type})\")\n",
        "    else:\n",
        "        print(\"INCORRECT\")\n",
        "\n",
        "    # Store the results for later analysis\n",
        "    results.append({\n",
        "        \"id\": example.get(\"id\"),\n",
        "        \"question\": question,\n",
        "        \"opa\": opa,\n",
        "        \"opb\": opb,\n",
        "        \"opc\": opc,\n",
        "        \"opd\": opd,\n",
        "        \"cop\": cop,\n",
        "        \"ground_truth\": ground_truth,\n",
        "        \"prediction\": prediction,\n",
        "        \"correct\": correct,\n",
        "        \"match_type\": match_type,\n",
        "    })\n",
        "\n",
        "    # Print running accuracy after each example\n",
        "    accuracy = (correct_exact + correct_partial) / i * 100\n",
        "    print(f\"Running accuracy: {accuracy:.1f}% \"\n",
        "          f\"({correct_exact + correct_partial}/{i} ; exact={correct_exact}, partial={correct_partial})\")\n",
        "\n",
        "# Total evaluation time\n",
        "total_time = time.time() - start_time\n",
        "print(f\"\\nFinished {n_total} examples in {total_time:.1f} seconds.\")\n",
        "print(f\"Final accuracy: {((correct_exact + correct_partial) / n_total) * 100:.1f}% \"\n",
        "      f\"(exact={correct_exact}, partial={correct_partial})\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y6xHyg6F1FRD",
        "outputId": "c28e0352-200f-49ab-a842-1cb761899e33"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "=== Evaluation summary ===\n",
            "Total: 6/20 correct (30.0%)\n",
            "  Exact matches   : 6\n",
            "  Partial matches : 0\n",
            "  Incorrect       : 14\n",
            "\n",
            "Total time   : 66.8s\n",
            "Per example  : 3.34s/example\n"
          ]
        }
      ],
      "source": [
        "n_total = len(selected_indices)\n",
        "n_correct = correct_exact + correct_partial\n",
        "n_incorrect = n_total - n_correct\n",
        "\n",
        "# Compute overall accuracy as a percentage\n",
        "accuracy = (n_correct / n_total) * 100 if n_total > 0 else 0.0\n",
        "\n",
        "# Print evaluation summary\n",
        "print(\"\\n=== Evaluation summary ===\")\n",
        "print(f\"Total: {n_correct}/{n_total} correct ({accuracy:.1f}%)\")\n",
        "print(f\"  Exact matches   : {correct_exact}\")\n",
        "print(f\"  Partial matches : {correct_partial}\")\n",
        "print(f\"  Incorrect       : {n_incorrect}\")\n",
        "\n",
        "# Print timing statistics if available\n",
        "if total_time > 0:\n",
        "    print(f\"\\nTotal time   : {total_time:.1f}s\")\n",
        "    print(f\"Per example  : {total_time / n_total:.2f}s/example\")\n",
        "else:\n",
        "    print(\"\\nTotal time   : 0.0s\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tb8uBl841Y46",
        "outputId": "800dad7d-4629-4b68-fd77-060fef07e261"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Results saved to: /content/evaluation_results.json\n"
          ]
        }
      ],
      "source": [
        "n_total = len(selected_indices)\n",
        "n_correct = correct_exact + correct_partial\n",
        "n_incorrect = n_total - n_correct\n",
        "\n",
        "# Prepare a dictionary summarizing the evaluation results\n",
        "results_summary = {\n",
        "    \"n_total\": n_total,                                                 # Total examples evaluated\n",
        "    \"accuracy\": accuracy,                                               # Overall accuracy (%)\n",
        "    \"exact_matches\": correct_exact,                                     # Number of exact matches\n",
        "    \"partial_matches\": correct_partial,                                 # Number of partial matches\n",
        "    \"incorrect\": n_incorrect,                                           # Number of incorrect predictions\n",
        "    \"total_time\": total_time,                                           # Total evaluation time in seconds\n",
        "    \"time_per_example\": total_time / n_total if n_total > 0 else None,  # Average time per example\n",
        "    \"selected_indices\": list(map(int, selected_indices)),     # List of evaluated indices\n",
        "    \"results\": results                                                  # List of individual example results\n",
        "}\n",
        "\n",
        "# Define the path for saving the evaluation results\n",
        "output_path = Path(\"evaluation_results.json\")\n",
        "\n",
        "# Save the results summary as a JSON file\n",
        "with output_path.open(\"w\", encoding=\"utf-8\") as f:\n",
        "    json.dump(results_summary, f, indent=2, ensure_ascii=False)\n",
        "\n",
        "# Confirm that results were saved\n",
        "print(f\"Results saved to: {output_path.resolve()}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Model Improvement Strategies"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "1. Based on your evaluation results, propose at least 2 or 3 specific strategies to improve your model's accuracy. For each strategy, explain what you would change, why it helps, and potential trade-offs."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Accuracy is low (30% on 20 questions), so the main levers are better supervision, better decoding, and better use of the base modelâ€™s capacity. Below are three concrete, defensible strategies.\n",
        "\n",
        "**Improve prompt formatting & decoding**\n",
        "\n",
        "- The evaluation shows many predictions that include extra text, question restatements, or numbered lists instead of a clean option or answer string, which then fail the matcher despite being semantically right in some cases.\n",
        "\n",
        "- Change:\n",
        "    - Constrain generation with a system/user instruction like â€œAnswer with exactly one option (A, B, C, or D) and nothing elseâ€.\n",
        "\n",
        "    - At evaluation time, post-process the model output to extract only the option letter or the answer span before comparing to the ground truth.\n",
        "\n",
        "- Why it helps: Reduces â€œformattingâ€ errors such as answering â€œ1. The answer is: Varicellaâ€ when the expected output is just â€œVaricellaâ€ or the option index.\n",
        "\n",
        "- Trade-offs:\n",
        "    - Slightly more engineering in the evaluation script and prompting.\n",
        "    - If the postâ€‘processing is brittle, it may misâ€‘parse some valid freeâ€‘form answers.\n",
        "\n",
        "**Increase & balance the fineâ€‘tuning data**\n",
        "\n",
        "- The model was fineâ€‘tuned only on 500 training examples, which is tiny compared with the difficulty and variety of MedMCQA questions seen in evaluation.\n",
        "\n",
        "- Change:\n",
        "    - Use many more MedMCQA training samples (e.g., the full train split or at least several thousand examples).\n",
        "\n",
        "    - Optionally upsample underâ€‘represented specialties or question types where errors are concentrated (e.g., oncology, ophthalmology, ENT), which appear repeatedly among wrong predictions.\n",
        "\n",
        "- Why it helps: More coverage of patterns and medical subdomains reduces random guessing and improves generalization across rare entities and treatments.\n",
        "\n",
        "- Tradeâ€‘offs:\n",
        "    - Higher compute and training time, possibly requiring more aggressive LoRA or lower max_length to fit in memory.\n",
        "    \n",
        "    - Risk of overfitting if training too long without validation monitoring.\n",
        "\n",
        "**Strengthen the supervision signal**\n",
        "\n",
        "- Currently, the model is trained with a standard causal LM objective over the whole prompt plus answer, which means it also learns to copy the question and template tokens rather than focusing purely on predicting the correct option.\n",
        "\n",
        "- Change:\n",
        "    - Mask the loss so that it is applied only on the answer segment (e.g., the â€œThe answer is: â€¦â€ part).\n",
        "\n",
        "    - Alternatively, reformat data so the target is just the option letter or short answer, and treat this as a shortâ€‘sequence generation problem.\n",
        "\n",
        "- Why it helps: Directly optimizes the part of the output that matters for accuracy, instead of wasting capacity on reconstructing the prompt.\n",
        "\n",
        "- Tradeâ€‘offs:\n",
        "    - Requires modifying the data collator or label construction logic.\n",
        "\n",
        "    - The model becomes specialized for this answer style and may be less fluent for longer generative explanations without additional fineâ€‘tuning.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "2. Review your incorrect predictions and identify patterns in failures. What can you tell about the model errors ?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The errors are mostly systematic rather than random guessing, with several recurring patterns in how the model fails.\n",
        "\n",
        "**Misclassification across plausible options**\n",
        "\n",
        "- In many cases the model chooses a medically plausible but wrong option (e.g., picking â€œVaricellaâ€ instead of â€œHIVâ€ for minimal teratogenic risk, or â€œEndometrial carcinomaâ€ instead of â€œRenal cell carcinomaâ€ for polycythemia).\n",
        "\n",
        "- This suggests limited factual grounding or confusion between related risk factors and tumor paraneoplastic associations rather than purely random outputs.\n",
        "\n",
        "**Confusion in â€œexcept / notâ€ style questions**\n",
        "\n",
        "- For questions framed as â€œall are true exceptâ€ or â€œwhich is not used,â€ the model often selects a true statement instead of the requested exception (e.g., xenon anesthesia question, agents not used in diabetic macular edema).\n",
        "\n",
        "- This indicates the model struggles with negation and exception reasoning, a common weakness in LMs, especially under fewâ€‘shot or weak supervision.\n",
        "\n",
        "**Domain gaps & rare facts**\n",
        "\n",
        "- Several errors are on niche topics (specific lymphoma epidemiology, named surgical procedures, unusual drug uses) where the correct answer is a relatively rare fact, and the model picks a more common entity instead.\n",
        "\n",
        "- This pattern suggests insufficient exposure during fineâ€‘tuning (only 500 examples) and reliance on prior generalâ€‘domain knowledge that is not specialized enough for fineâ€‘grained medical trivia."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "3. What do you think it's better between training on 2000 examples (same quality) or 500 curated high-quality examples ?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "With the same label quality and format, 2000 reasonably good examples are typically better for this task than 500 curated ones, given your current low data regime and broad domain coverage needs.\n",
        "\n",
        "**Why 2000 > 500 here**\n",
        "\n",
        "- The model currently sees only 500 MedMCQA questions while being evaluated on diverse, fineâ€‘grained medical facts (tumors, infections, anesthesia, etc.), and many errors look like domainâ€‘coverage gaps rather than noise from bad labels.\n",
        "\n",
        "- Adding more examples (up to 2000) improves coverage of specialties and question templates, which is crucial for a general medical MCQ model, and LoRA can handle that scale easily on our setup.\n",
        "\n",
        "**When 500 curated could win**\n",
        "\n",
        "- If â€œcuratedâ€ means carefully chosen hard/representative items that target known weaknesses (negation, â€œexceptâ€ questions, rare entities) and the alternative 2000 set contains substantial label noise or badly formatted prompts, the smaller curated set can yield better generalization per example.â€‹\n",
        "\n",
        "- However, our notebook already filters out obviously lowâ€‘quality items (short questions, missing answers), and there is no strong evidence of heavy label noise, so in this specific setup the larger 2000â€‘example training set is the safer choice."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Resource-Constrained Inference"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "4. How can you design a strategy to reduce inference time/memory for deployment in constrained environments ?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "A good strategy is to combine parameterâ€‘efficient fineâ€‘tuning (LoRA), quantization, and hardwareâ€‘aware deployment so that only a small, compressed adapterâ€‘augmented model is served.\n",
        "\n",
        "**Use LoRA & freeze the base**\n",
        "\n",
        "- Keep the current setup where only ~1% of parameters are trainable adapters and the base SmolLM2 model is frozen.\n",
        "\n",
        "- At deployment, we load the pretrained base weights once and merge or attach only the small LoRA layers, which reduces the size of the fineâ€‘tuned delta we need to ship and store.\n",
        "\n",
        "**Apply postâ€‘training quantization**\n",
        "\n",
        "- Quantize the model to 8â€‘bit or 4â€‘bit weights (e.g., with bitsandbytes or similar libraries) so that parameters and activations use fewer bytes, lowering both memory footprint and bandwidth.\n",
        "\n",
        "- This typically yields large memory and latency savings with modest accuracy loss, especially on relatively small models like 1.7B parameters.\n",
        "\n",
        "**Optimize decoding & batching**\n",
        "\n",
        "- Constrain generation for MCQs (e.g., short max_length, greedy or lowâ€‘beam search) since you only need one option, which cuts perâ€‘request compute.\n",
        "\n",
        "- If the use case allows, batch multiple questions per forward pass and reuse the same loaded model instance, improving throughput without increasing model size."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "5. Analyze how changing generation parameters affects speed, quality, and consistency."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Generation parameters trade off speed, output quality, and consistency in predictable ways for your MCQ setting.\n",
        "\n",
        "**Max length & early stopping**\n",
        "\n",
        "- Lower `max_new_tokens` (or `max_length`) speeds up inference almost linearly, because fewer tokens are generated, but can truncate explanations or long answers if set too low.\n",
        "\n",
        "- In your MCQ task, a small limit (e.g., 10â€“20 tokens) is usually enough and improves latency without hurting accuracy, since only a short option/phrase is needed.\n",
        "\n",
        "**Decoding strategy (greedy vs sampling vs beam)**\n",
        "\n",
        "- Greedy decoding (no sampling, `top_p`/`temperature` off) is fastest and most consistent because each step picks the argmax token, but may be slightly less robust if the modelâ€™s nextâ€‘token distribution is poorly calibrated.\n",
        "\n",
        "- Sampling with higher `temperature` or `top_p` increases diversity and can improve answer quality in openâ€‘ended tasks, but here it mainly adds randomness and reduces answer consistency across runs.\n",
        "\n",
        "- Beam search can improve exactâ€‘match quality by exploring multiple candidate sequences but increases latency roughly proportional to beam size and is often unnecessary when the desired output is a single short label.\n",
        "\n",
        "**Temperature, topâ€‘k & topâ€‘p**\n",
        "\n",
        "- Lower `temperature` (e.g., 0.1â€“0.3) and small `top_p`/`top_k` make outputs more deterministic and regular, improving consistency across repeated calls but possibly locking in systematic biases or mistakes.â€‹\n",
        "\n",
        "- Higher `temperature` or larger `top_p`/`top_k` allow exploration, which can occasionally find the correct option when the greedy path is wrong, but at the cost of unstable answers and slower average decoding if more tokens are sampled before reaching an end token.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Evaluation Methodology"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "6. Analyze limitations of current exact/partial match evaluation and propose improvements. Do you think you have false negatives or false positives ? What can we do about it ?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The current evaluation is prone mainly to false negatives (correct or acceptable answers marked wrong) because it relies on brittle string matching of fairly freeâ€‘form generations.\n",
        "\n",
        "**Limitations & likely false negatives**\n",
        "\n",
        "- Many outputs contain the right answer embedded in extra text or formatting, such as â€œ1. The answer is: Varicellaâ€ when the ground truth is â€œVaricellaâ€; these are counted as incorrect because the evaluation expects an exact string or tightly constrained form.\n",
        "\n",
        "- Some predictions restate or slightly rephrase the option text (e.g., â€œlymphocyteâ€‘predominant HDâ€ vs â€œlymphocyteâ€‘predominant Hodgkin disease (HD)â€), which are semantically equivalent but scored as noâ€‘match due to surface differences.\n",
        "\n",
        "**Possible false positives**\n",
        "\n",
        "- There are fewer indications of false positives, but they can occur if the evaluation only checks for the presence of the correct string anywhere in the output; in that case, a long explanation that mentions multiple options could be misâ€‘scored as correct even if the final choice is wrong.\n",
        "\n",
        "- For example, a model might list all options or discuss them and incidentally include the correct phrase without clearly selecting it, which a naive substring check might accept.\n",
        "\n",
        "**Proposed improvements**\n",
        "\n",
        "- Constrain generation & labels: Force the model to answer with a single letter (A/B/C/D) or a fixed pattern like â€œAnswer: Aâ€, and store ground truth as the option index; evaluation then compares normalized letters instead of full strings.\n",
        "\n",
        "- Robust postâ€‘processing: Strip prefixes like â€œ1.â€, â€œThe answer is:â€, and surrounding punctuation; extract the final option letter or answer span using regex/heuristics before matching.\n",
        "\n",
        "- Semantic or ruleâ€‘based matching: Normalize case, remove parentheses/abbreviations, and allow small edit distances so that â€œlymphocyteâ€‘predominant HDâ€ matches the longer canonical form.\n",
        "\n",
        "- Human spotâ€‘check: For a small sample, manually inspect â€œincorrectâ€ and â€œpartialâ€ cases to quantify how often the model is actually right but misâ€‘scored, then refine the matching rules accordingly.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "7. Test other test size and observe the result. What can you say about the results ? How can you improve it ?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "With the current setup (20â€‘question sample, 30% accuracy), the test size is very small and the metric is noisy; increasing or varying test size will mainly stabilize the estimate and reveal variance across question subsets.\n",
        "\n",
        "**What changing test size shows**\n",
        "\n",
        "- On only 20 items, each individual question shifts accuracy by 5 percentage points, so small changes in sampled indices can give quite different reported performance, especially with many borderline cases and formattingâ€‘sensitive scoring.\n",
        "\n",
        "- Using larger test sizes (e.g., 50, 100, 200 questions) will smooth out this sampling noise and give a more reliable picture of true accuracy, and may show that performance varies by topic (e.g., pediatrics vs oncology vs ENT).\n",
        "\n",
        "**How to improve the evaluation ?**\n",
        "\n",
        "- Increase `n_total` in our evaluation script to test on more randomly sampled questions (or the full validation/test split) so that metrics are statistically more stable.\n",
        "\n",
        "- Stratify the test set by specialty or question type so you can see systematic weaknesses instead of a single aggregate accuracy, then focus fineâ€‘tuning and prompt design on those weak categories."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Real-World Deployment Scenario"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "8. What can you do to address safety, reliability, updates, and edge cases for deploying in a medical assistance application ?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "For deployment as a medical assistance tool, you would need a multiâ€‘layer approach: strict scope limits, guardrails around the model, continuous validation, and clear userâ€‘facing safety mechanisms.\n",
        "\n",
        "**Safety & scope**\n",
        "\n",
        "- Explicitly restrict the modelâ€™s scope to educational/decisionâ€‘support use, with prompts and system instructions that forbid giving definitive diagnoses, prescriptions, or emergency instructions; always include a disclaimer and encourage consulting a clinician.\n",
        "\n",
        "- Add ruleâ€‘based and learned filters that block or heavily qualify outputs in sensitive areas (e.g., dosing, oncologic treatment plans, pregnancyâ€‘related advice, pediatrics) and instead respond with safe guidance like â€œconsult a specialist.â€\n",
        "\n",
        "**Reliability & evaluation**\n",
        "\n",
        "- Before deployment, run largeâ€‘scale evaluation on curated medical benchmarks and internal test sets, including manual review by domain experts, especially for highâ€‘risk topics where our 30% accuracy shows current unreliability.â€‹\n",
        "\n",
        "- In production, implement monitoring: log queries and sampled outputs (with proper privacy protections), track error reports and adverse feedback, and regularly reâ€‘evaluate performance to detect regressions over time.\n",
        "\n",
        "**Updates & versioning**\n",
        "\n",
        "- Maintain a clear model versioning scheme (base model version, fineâ€‘tune date, dataset snapshot) so that any change to guidelines or medical knowledge results in a new, traceable release.\n",
        "\n",
        "- Periodically update training data with new guidelines, drug warnings, and protocols, and run regression tests to ensure that improvements in one area do not degrade performance elsewhere.\n",
        "\n",
        "**Handling edge cases**\n",
        "\n",
        "- Detect outâ€‘ofâ€‘scope or ambiguous queries (e.g., very rare diseases, incomplete clinical context, medicoâ€‘legal questions) via confidence estimates or auxiliary classifiers, and respond by asking for clarification or deferring to human clinicians rather than improvising.\n",
        "\n",
        "- For critical domains (e.g., emergency medicine, intensive care), design workflows that require human approval: the model can propose differential diagnoses or questions to ask, but a clinician must review and confirm any actionable recommendation."
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "075a43deb7fd49d18d6a7cf27d0ff6ce": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "3e9a6c10c163477aa189825c16b214a1": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4caee023b2394d3f9c0ac6f8570e8e59": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3e9a6c10c163477aa189825c16b214a1",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_4e1414f19bd2452ead1559c4c2835b28",
            "value": "Map:â€‡100%"
          }
        },
        "4e1414f19bd2452ead1559c4c2835b28": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "57865505e22040a6afd2c9968fa274f8": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "74e23ea62c764f9cbaddd9e3ea6ce210": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7b356cfd70cb45fcb7b7dd32c46e4c0d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_4caee023b2394d3f9c0ac6f8570e8e59",
              "IPY_MODEL_e3dd67f3d525454db3b766fe10e94243",
              "IPY_MODEL_eb53e4d4ec4f4a71a6f3876d39448780"
            ],
            "layout": "IPY_MODEL_fdcc5276ad9242eda8d19fbd07a02511"
          }
        },
        "a732e1b91f9a4046b5cdb84f66d75ac6": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e3dd67f3d525454db3b766fe10e94243": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_74e23ea62c764f9cbaddd9e3ea6ce210",
            "max": 500,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_075a43deb7fd49d18d6a7cf27d0ff6ce",
            "value": 500
          }
        },
        "eb53e4d4ec4f4a71a6f3876d39448780": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_57865505e22040a6afd2c9968fa274f8",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_a732e1b91f9a4046b5cdb84f66d75ac6",
            "value": "â€‡500/500â€‡[00:00&lt;00:00,â€‡598.16â€‡examples/s]"
          }
        },
        "fdcc5276ad9242eda8d19fbd07a02511": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
